<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "c1/c1_Compilation.hpp"
  29 #include "c1/c1_LIRAssembler.hpp"
  30 #include "c1/c1_MacroAssembler.hpp"
  31 #include "c1/c1_Runtime1.hpp"
  32 #include "c1/c1_ValueStack.hpp"
  33 #include "ci/ciArrayKlass.hpp"
  34 #include "ci/ciInstance.hpp"
  35 #include "gc/shared/barrierSet.hpp"
  36 #include "gc/shared/cardTableBarrierSet.hpp"
  37 #include "gc/shared/collectedHeap.hpp"
  38 #include "nativeInst_x86.hpp"
  39 #include "oops/objArrayKlass.hpp"
  40 #include "runtime/frame.inline.hpp"
  41 #include "runtime/safepointMechanism.hpp"
  42 #include "runtime/sharedRuntime.hpp"
  43 #include "vmreg_x86.inline.hpp"
  44 
  45 
  46 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  47 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  48 // fast versions of NegF/NegD and AbsF/AbsD.
  49 
  50 // Note: 'double' and 'long long' have 32-bits alignment on x86.
  51 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  52   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  53   // of 128-bits operands for SSE instructions.
  54   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  55   // Store the value to a 128-bits operand.
  56   operand[0] = lo;
  57   operand[1] = hi;
  58   return operand;
  59 }
  60 
  61 // Buffer for 128-bits masks used by SSE instructions.
  62 static jlong fp_signmask_pool[(4+1)*2]; // 4*128bits(data) + 128bits(alignment)
  63 
  64 // Static initialization during VM startup.
  65 static jlong *float_signmask_pool  = double_quadword(&amp;fp_signmask_pool[1*2],         CONST64(0x7FFFFFFF7FFFFFFF),         CONST64(0x7FFFFFFF7FFFFFFF));
  66 static jlong *double_signmask_pool = double_quadword(&amp;fp_signmask_pool[2*2],         CONST64(0x7FFFFFFFFFFFFFFF),         CONST64(0x7FFFFFFFFFFFFFFF));
  67 static jlong *float_signflip_pool  = double_quadword(&amp;fp_signmask_pool[3*2], (jlong)UCONST64(0x8000000080000000), (jlong)UCONST64(0x8000000080000000));
  68 static jlong *double_signflip_pool = double_quadword(&amp;fp_signmask_pool[4*2], (jlong)UCONST64(0x8000000000000000), (jlong)UCONST64(0x8000000000000000));
  69 
  70 
  71 NEEDS_CLEANUP // remove this definitions ?
  72 const Register IC_Klass    = rax;   // where the IC klass is cached
  73 const Register SYNC_header = rax;   // synchronization header
  74 const Register SHIFT_count = rcx;   // where count for shift operations must be
  75 
  76 #define __ _masm-&gt;
  77 
  78 
  79 static void select_different_registers(Register preserve,
  80                                        Register extra,
  81                                        Register &amp;tmp1,
  82                                        Register &amp;tmp2) {
  83   if (tmp1 == preserve) {
  84     assert_different_registers(tmp1, tmp2, extra);
  85     tmp1 = extra;
  86   } else if (tmp2 == preserve) {
  87     assert_different_registers(tmp1, tmp2, extra);
  88     tmp2 = extra;
  89   }
  90   assert_different_registers(preserve, tmp1, tmp2);
  91 }
  92 
  93 
  94 
  95 static void select_different_registers(Register preserve,
  96                                        Register extra,
  97                                        Register &amp;tmp1,
  98                                        Register &amp;tmp2,
  99                                        Register &amp;tmp3) {
 100   if (tmp1 == preserve) {
 101     assert_different_registers(tmp1, tmp2, tmp3, extra);
 102     tmp1 = extra;
 103   } else if (tmp2 == preserve) {
 104     assert_different_registers(tmp1, tmp2, tmp3, extra);
 105     tmp2 = extra;
 106   } else if (tmp3 == preserve) {
 107     assert_different_registers(tmp1, tmp2, tmp3, extra);
 108     tmp3 = extra;
 109   }
 110   assert_different_registers(preserve, tmp1, tmp2, tmp3);
 111 }
 112 
 113 
 114 
 115 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
 116   if (opr-&gt;is_constant()) {
 117     LIR_Const* constant = opr-&gt;as_constant_ptr();
 118     switch (constant-&gt;type()) {
 119       case T_INT: {
 120         return true;
 121       }
 122 
 123       default:
 124         return false;
 125     }
 126   }
 127   return false;
 128 }
 129 
 130 
 131 LIR_Opr LIR_Assembler::receiverOpr() {
 132   return FrameMap::receiver_opr;
 133 }
 134 
 135 LIR_Opr LIR_Assembler::osrBufferPointer() {
 136   return FrameMap::as_pointer_opr(receiverOpr()-&gt;as_register());
 137 }
 138 
 139 //--------------fpu register translations-----------------------
 140 
 141 
 142 address LIR_Assembler::float_constant(float f) {
 143   address const_addr = __ float_constant(f);
 144   if (const_addr == NULL) {
 145     bailout("const section overflow");
 146     return __ code()-&gt;consts()-&gt;start();
 147   } else {
 148     return const_addr;
 149   }
 150 }
 151 
 152 
 153 address LIR_Assembler::double_constant(double d) {
 154   address const_addr = __ double_constant(d);
 155   if (const_addr == NULL) {
 156     bailout("const section overflow");
 157     return __ code()-&gt;consts()-&gt;start();
 158   } else {
 159     return const_addr;
 160   }
 161 }
 162 
 163 
 164 void LIR_Assembler::set_24bit_FPU() {
 165   __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_24()));
 166 }
 167 
 168 void LIR_Assembler::reset_FPU() {
 169   __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
 170 }
 171 
 172 void LIR_Assembler::fpop() {
 173   __ fpop();
 174 }
 175 
 176 void LIR_Assembler::fxch(int i) {
 177   __ fxch(i);
 178 }
 179 
 180 void LIR_Assembler::fld(int i) {
 181   __ fld_s(i);
 182 }
 183 
 184 void LIR_Assembler::ffree(int i) {
 185   __ ffree(i);
 186 }
 187 
 188 void LIR_Assembler::breakpoint() {
 189   __ int3();
 190 }
 191 
 192 void LIR_Assembler::push(LIR_Opr opr) {
 193   if (opr-&gt;is_single_cpu()) {
 194     __ push_reg(opr-&gt;as_register());
 195   } else if (opr-&gt;is_double_cpu()) {
 196     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 197     __ push_reg(opr-&gt;as_register_lo());
 198   } else if (opr-&gt;is_stack()) {
 199     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 200   } else if (opr-&gt;is_constant()) {
 201     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
 202     if (const_opr-&gt;type() == T_OBJECT) {
 203       __ push_oop(const_opr-&gt;as_jobject());
 204     } else if (const_opr-&gt;type() == T_INT) {
 205       __ push_jint(const_opr-&gt;as_jint());
 206     } else {
 207       ShouldNotReachHere();
 208     }
 209 
 210   } else {
 211     ShouldNotReachHere();
 212   }
 213 }
 214 
 215 void LIR_Assembler::pop(LIR_Opr opr) {
 216   if (opr-&gt;is_single_cpu()) {
 217     __ pop_reg(opr-&gt;as_register());
 218   } else {
 219     ShouldNotReachHere();
 220   }
 221 }
 222 
 223 bool LIR_Assembler::is_literal_address(LIR_Address* addr) {
 224   return addr-&gt;base()-&gt;is_illegal() &amp;&amp; addr-&gt;index()-&gt;is_illegal();
 225 }
 226 
 227 //-------------------------------------------
 228 
 229 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 230   return as_Address(addr, rscratch1);
 231 }
 232 
 233 Address LIR_Assembler::as_Address(LIR_Address* addr, Register tmp) {
 234   if (addr-&gt;base()-&gt;is_illegal()) {
 235     assert(addr-&gt;index()-&gt;is_illegal(), "must be illegal too");
 236     AddressLiteral laddr((address)addr-&gt;disp(), relocInfo::none);
 237     if (! __ reachable(laddr)) {
 238       __ movptr(tmp, laddr.addr());
 239       Address res(tmp, 0);
 240       return res;
 241     } else {
 242       return __ as_Address(laddr);
 243     }
 244   }
 245 
 246   Register base = addr-&gt;base()-&gt;as_pointer_register();
 247 
 248   if (addr-&gt;index()-&gt;is_illegal()) {
 249     return Address( base, addr-&gt;disp());
 250   } else if (addr-&gt;index()-&gt;is_cpu_register()) {
 251     Register index = addr-&gt;index()-&gt;as_pointer_register();
 252     return Address(base, index, (Address::ScaleFactor) addr-&gt;scale(), addr-&gt;disp());
 253   } else if (addr-&gt;index()-&gt;is_constant()) {
 254     intptr_t addr_offset = (addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() &lt;&lt; addr-&gt;scale()) + addr-&gt;disp();
 255     assert(Assembler::is_simm32(addr_offset), "must be");
 256 
 257     return Address(base, addr_offset);
 258   } else {
 259     Unimplemented();
 260     return Address();
 261   }
 262 }
 263 
 264 
 265 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 266   Address base = as_Address(addr);
 267   return Address(base._base, base._index, base._scale, base._disp + BytesPerWord);
 268 }
 269 
 270 
 271 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 272   return as_Address(addr);
 273 }
 274 
 275 
 276 void LIR_Assembler::osr_entry() {
 277   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 278   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 279   ValueStack* entry_state = osr_entry-&gt;state();
 280   int number_of_locks = entry_state-&gt;locks_size();
 281 
 282   // we jump here if osr happens with the interpreter
 283   // state set up to continue at the beginning of the
 284   // loop that triggered osr - in particular, we have
 285   // the following registers setup:
 286   //
 287   // rcx: osr buffer
 288   //
 289 
 290   // build frame
 291   ciMethod* m = compilation()-&gt;method();
 292   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 293 
 294   // OSR buffer is
 295   //
 296   // locals[nlocals-1..0]
 297   // monitors[0..number_of_locks]
 298   //
 299   // locals is a direct copy of the interpreter frame so in the osr buffer
 300   // so first slot in the local array is the last local from the interpreter
 301   // and last slot is local[0] (receiver) from the interpreter
 302   //
 303   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 304   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 305   // in the interpreter frame (the method lock if a sync method)
 306 
 307   // Initialize monitors in the compiled activation.
 308   //   rcx: pointer to osr buffer
 309   //
 310   // All other registers are dead at this point and the locals will be
 311   // copied into place by code emitted in the IR.
 312 
 313   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 314   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), "adjust code below");
 315     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 316       (BasicObjectLock::size() * BytesPerWord) * (number_of_locks - 1);
 317     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 318     // the OSR buffer using 2 word entries: first the lock and then
 319     // the oop.
 320     for (int i = 0; i &lt; number_of_locks; i++) {
 321       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 322 #ifdef ASSERT
 323       // verify the interpreter's monitor has a non-null object
 324       {
 325         Label L;
 326         __ cmpptr(Address(OSR_buf, slot_offset + 1*BytesPerWord), (int32_t)NULL_WORD);
 327         __ jcc(Assembler::notZero, L);
 328         __ stop("locked object is NULL");
 329         __ bind(L);
 330       }
 331 #endif
 332       __ movptr(rbx, Address(OSR_buf, slot_offset + 0));
 333       __ movptr(frame_map()-&gt;address_for_monitor_lock(i), rbx);
 334       __ movptr(rbx, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 335       __ movptr(frame_map()-&gt;address_for_monitor_object(i), rbx);
 336     }
 337   }
 338 }
 339 
 340 
 341 // inline cache check; done before the frame is built.
 342 int LIR_Assembler::check_icache() {
 343   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 344   Register ic_klass = IC_Klass;
 345   const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);
 346   const bool do_post_padding = VerifyOops || UseCompressedClassPointers;
 347   if (!do_post_padding) {
 348     // insert some nops so that the verified entry point is aligned on CodeEntryAlignment
 349     __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);
 350   }
 351   int offset = __ offset();
 352   __ inline_cache_check(receiver, IC_Klass);
 353   assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, "alignment must be correct");
 354   if (do_post_padding) {
 355     // force alignment after the cache check.
 356     // It's been verified to be aligned if !VerifyOops
 357     __ align(CodeEntryAlignment);
 358   }
 359   return offset;
 360 }
 361 
 362 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 363   assert(VM_Version::supports_fast_class_init_checks(), "sanity");
 364   assert(!method-&gt;holder()-&gt;is_not_initialized(), "initialization should have been started");
 365 
 366   Label L_skip_barrier;
 367   Register klass = rscratch1;
 368   Register thread = LP64_ONLY( r15_thread ) NOT_LP64( noreg );
 369   assert(thread != noreg, "x86_32 not implemented");
 370 
 371   __ mov_metadata(klass, method-&gt;holder()-&gt;constant_encoding());
 372   __ clinit_barrier(klass, thread, &amp;L_skip_barrier /*L_fast_path*/);
 373 
 374   __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
 375 
 376   __ bind(L_skip_barrier);
 377 }
 378 
 379 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 380   jobject o = NULL;
 381   PatchingStub* patch = new PatchingStub(_masm, patching_id(info));
 382   __ movoop(reg, o);
 383   patching_epilog(patch, lir_patch_normal, reg, info);
 384 }
 385 
 386 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 387   Metadata* o = NULL;
 388   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id);
 389   __ mov_metadata(reg, o);
 390   patching_epilog(patch, lir_patch_normal, reg, info);
 391 }
 392 
 393 // This specifies the rsp decrement needed to build the frame
 394 int LIR_Assembler::initial_frame_size_in_bytes() const {
 395   // if rounding, must let FrameMap know!
 396 
 397   // The frame_map records size in slots (32bit word)
 398 
 399   // subtract two words to account for return address and link
 400   return (frame_map()-&gt;framesize() - (2*VMRegImpl::slots_per_word))  * VMRegImpl::stack_slot_size;
 401 }
 402 
 403 
 404 int LIR_Assembler::emit_exception_handler() {
 405   // if the last instruction is a call (typically to do a throw which
 406   // is coming at the end after block reordering) the return address
 407   // must still point into the code area in order to avoid assertion
 408   // failures when searching for the corresponding bci =&gt; add a nop
 409   // (was bug 5/14/1999 - gri)
 410   __ nop();
 411 
 412   // generate code for exception handler
 413   address handler_base = __ start_a_stub(exception_handler_size());
 414   if (handler_base == NULL) {
 415     // not enough space left for the handler
 416     bailout("exception handler overflow");
 417     return -1;
 418   }
 419 
 420   int offset = code_offset();
 421 
 422   // the exception oop and pc are in rax, and rdx
 423   // no other registers need to be preserved, so invalidate them
 424   __ invalidate_registers(false, true, true, false, true, true);
 425 
 426   // check that there is really an exception
 427   __ verify_not_null_oop(rax);
 428 
 429   // search an exception handler (rax: exception oop, rdx: throwing pc)
 430   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));
 431   __ should_not_reach_here();
 432   guarantee(code_offset() - offset &lt;= exception_handler_size(), "overflow");
 433   __ end_a_stub();
 434 
 435   return offset;
 436 }
 437 
 438 
 439 // Emit the code to remove the frame from the stack in the exception
 440 // unwind path.
 441 int LIR_Assembler::emit_unwind_handler() {
 442 #ifndef PRODUCT
 443   if (CommentedAssembly) {
 444     _masm-&gt;block_comment("Unwind handler");
 445   }
 446 #endif
 447 
 448   int offset = code_offset();
 449 
 450   // Fetch the exception from TLS and clear out exception related thread state
 451   Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
 452   NOT_LP64(__ get_thread(rsi));
 453   __ movptr(rax, Address(thread, JavaThread::exception_oop_offset()));
 454   __ movptr(Address(thread, JavaThread::exception_oop_offset()), (intptr_t)NULL_WORD);
 455   __ movptr(Address(thread, JavaThread::exception_pc_offset()), (intptr_t)NULL_WORD);
 456 
 457   __ bind(_unwind_handler_entry);
 458   __ verify_not_null_oop(rax);
 459   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 460     __ mov(rbx, rax);  // Preserve the exception (rbx is always callee-saved)
 461   }
 462 
 463   // Preform needed unlocking
 464   MonitorExitStub* stub = NULL;
 465   if (method()-&gt;is_synchronized()) {
 466     monitor_address(0, FrameMap::rax_opr);
 467     stub = new MonitorExitStub(FrameMap::rax_opr, true, 0);
 468     __ unlock_object(rdi, rsi, rax, *stub-&gt;entry());
 469     __ bind(*stub-&gt;continuation());
 470   }
 471 
 472   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 473 #ifdef _LP64
 474     __ mov(rdi, r15_thread);
 475     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 476 #else
 477     __ get_thread(rax);
 478     __ movptr(Address(rsp, 0), rax);
 479     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());
 480 #endif
 481     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 482   }
 483 
 484   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 485     __ mov(rax, rbx);  // Restore the exception
 486   }
 487 
 488   // remove the activation and dispatch to the unwind handler
 489   __ remove_frame(initial_frame_size_in_bytes());
 490   __ jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 491 
 492   // Emit the slow path assembly
 493   if (stub != NULL) {
 494     stub-&gt;emit_code(this);
 495   }
 496 
 497   return offset;
 498 }
 499 
 500 
 501 int LIR_Assembler::emit_deopt_handler() {
 502   // if the last instruction is a call (typically to do a throw which
 503   // is coming at the end after block reordering) the return address
 504   // must still point into the code area in order to avoid assertion
 505   // failures when searching for the corresponding bci =&gt; add a nop
 506   // (was bug 5/14/1999 - gri)
 507   __ nop();
 508 
 509   // generate code for exception handler
 510   address handler_base = __ start_a_stub(deopt_handler_size());
 511   if (handler_base == NULL) {
 512     // not enough space left for the handler
 513     bailout("deopt handler overflow");
 514     return -1;
 515   }
 516 
 517   int offset = code_offset();
 518   InternalAddress here(__ pc());
 519 
 520   __ pushptr(here.addr());
 521   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 522   guarantee(code_offset() - offset &lt;= deopt_handler_size(), "overflow");
 523   __ end_a_stub();
 524 
 525   return offset;
 526 }
 527 
 528 
 529 void LIR_Assembler::return_op(LIR_Opr result) {
 530   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == rax, "word returns are in rax,");
 531   if (!result-&gt;is_illegal() &amp;&amp; result-&gt;is_float_kind() &amp;&amp; !result-&gt;is_xmm_register()) {
 532     assert(result-&gt;fpu() == 0, "result must already be on TOS");
 533   }
 534 
 535   // Pop the stack before the safepoint code
 536   __ remove_frame(initial_frame_size_in_bytes());
 537 
 538   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 539     __ reserved_stack_check();
 540   }
 541 
 542   bool result_is_oop = result-&gt;is_valid() ? result-&gt;is_oop() : false;
 543 
 544   // Note: we do not need to round double result; float result has the right precision
 545   // the poll sets the condition code, but no data registers
 546 
 547   if (SafepointMechanism::uses_thread_local_poll()) {
 548 #ifdef _LP64
 549     const Register poll_addr = rscratch1;
 550     __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 551 #else
 552     const Register poll_addr = rbx;
 553     assert(FrameMap::is_caller_save_register(poll_addr), "will overwrite");
 554     __ get_thread(poll_addr);
 555     __ movptr(poll_addr, Address(poll_addr, Thread::polling_page_offset()));
 556 #endif
 557     __ relocate(relocInfo::poll_return_type);
 558     __ testl(rax, Address(poll_addr, 0));
 559   } else {
 560     AddressLiteral polling_page(os::get_polling_page(), relocInfo::poll_return_type);
 561 
 562     if (Assembler::is_polling_page_far()) {
 563       __ lea(rscratch1, polling_page);
 564       __ relocate(relocInfo::poll_return_type);
 565       __ testl(rax, Address(rscratch1, 0));
 566     } else {
 567       __ testl(rax, polling_page);
 568     }
 569   }
 570   __ ret(0);
 571 }
 572 
 573 
 574 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 575   guarantee(info != NULL, "Shouldn't be NULL");
 576   int offset = __ offset();
 577   if (SafepointMechanism::uses_thread_local_poll()) {
 578 #ifdef _LP64
 579     const Register poll_addr = rscratch1;
 580     __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 581 #else
 582     assert(tmp-&gt;is_cpu_register(), "needed");
 583     const Register poll_addr = tmp-&gt;as_register();
 584     __ get_thread(poll_addr);
 585     __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));
 586 #endif
 587     add_debug_info_for_branch(info);
 588     __ relocate(relocInfo::poll_type);
 589     address pre_pc = __ pc();
 590     __ testl(rax, Address(poll_addr, 0));
 591     address post_pc = __ pc();
 592     guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), "must be exact length");
 593   } else {
 594     AddressLiteral polling_page(os::get_polling_page(), relocInfo::poll_type);
 595     if (Assembler::is_polling_page_far()) {
 596       __ lea(rscratch1, polling_page);
 597       offset = __ offset();
 598       add_debug_info_for_branch(info);
 599       __ relocate(relocInfo::poll_type);
 600       __ testl(rax, Address(rscratch1, 0));
 601     } else {
 602       add_debug_info_for_branch(info);
 603       __ testl(rax, polling_page);
 604     }
 605   }
 606   return offset;
 607 }
 608 
 609 
 610 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 611   if (from_reg != to_reg) __ mov(to_reg, from_reg);
 612 }
 613 
 614 void LIR_Assembler::swap_reg(Register a, Register b) {
 615   __ xchgptr(a, b);
 616 }
 617 
 618 
 619 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 620   assert(src-&gt;is_constant(), "should not call otherwise");
 621   assert(dest-&gt;is_register(), "should not call otherwise");
 622   LIR_Const* c = src-&gt;as_constant_ptr();
 623 
 624   switch (c-&gt;type()) {
 625     case T_INT: {
 626       assert(patch_code == lir_patch_none, "no patching handled here");
 627       __ movl(dest-&gt;as_register(), c-&gt;as_jint());
 628       break;
 629     }
 630 
 631     case T_ADDRESS: {
 632       assert(patch_code == lir_patch_none, "no patching handled here");
 633       __ movptr(dest-&gt;as_register(), c-&gt;as_jint());
 634       break;
 635     }
 636 
 637     case T_LONG: {
 638       assert(patch_code == lir_patch_none, "no patching handled here");
 639 #ifdef _LP64
 640       __ movptr(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 641 #else
 642       __ movptr(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 643       __ movptr(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 644 #endif // _LP64
 645       break;
 646     }
 647 
 648     case T_OBJECT: {
 649       if (patch_code != lir_patch_none) {
 650         jobject2reg_with_patching(dest-&gt;as_register(), info);
 651       } else {
 652         __ movoop(dest-&gt;as_register(), c-&gt;as_jobject());
 653       }
 654       break;
 655     }
 656 
 657     case T_METADATA: {
 658       if (patch_code != lir_patch_none) {
 659         klass2reg_with_patching(dest-&gt;as_register(), info);
 660       } else {
 661         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 662       }
 663       break;
 664     }
 665 
 666     case T_FLOAT: {
 667       if (dest-&gt;is_single_xmm()) {
 668         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_float()) {
 669           __ xorps(dest-&gt;as_xmm_float_reg(), dest-&gt;as_xmm_float_reg());
 670         } else {
 671           __ movflt(dest-&gt;as_xmm_float_reg(),
 672                    InternalAddress(float_constant(c-&gt;as_jfloat())));
 673         }
 674       } else {
 675         assert(dest-&gt;is_single_fpu(), "must be");
 676         assert(dest-&gt;fpu_regnr() == 0, "dest must be TOS");
 677         if (c-&gt;is_zero_float()) {
 678           __ fldz();
 679         } else if (c-&gt;is_one_float()) {
 680           __ fld1();
 681         } else {
 682           __ fld_s (InternalAddress(float_constant(c-&gt;as_jfloat())));
 683         }
 684       }
 685       break;
 686     }
 687 
 688     case T_DOUBLE: {
 689       if (dest-&gt;is_double_xmm()) {
 690         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_double()) {
 691           __ xorpd(dest-&gt;as_xmm_double_reg(), dest-&gt;as_xmm_double_reg());
 692         } else {
 693           __ movdbl(dest-&gt;as_xmm_double_reg(),
 694                     InternalAddress(double_constant(c-&gt;as_jdouble())));
 695         }
 696       } else {
 697         assert(dest-&gt;is_double_fpu(), "must be");
 698         assert(dest-&gt;fpu_regnrLo() == 0, "dest must be TOS");
 699         if (c-&gt;is_zero_double()) {
 700           __ fldz();
 701         } else if (c-&gt;is_one_double()) {
 702           __ fld1();
 703         } else {
 704           __ fld_d (InternalAddress(double_constant(c-&gt;as_jdouble())));
 705         }
 706       }
 707       break;
 708     }
 709 
 710     default:
 711       ShouldNotReachHere();
 712   }
 713 }
 714 
 715 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 716   assert(src-&gt;is_constant(), "should not call otherwise");
 717   assert(dest-&gt;is_stack(), "should not call otherwise");
 718   LIR_Const* c = src-&gt;as_constant_ptr();
 719 
 720   switch (c-&gt;type()) {
 721     case T_INT:  // fall through
 722     case T_FLOAT:
 723       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 724       break;
 725 
 726     case T_ADDRESS:
 727       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 728       break;
 729 
 730     case T_OBJECT:
 731       __ movoop(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jobject());
 732       break;
 733 
 734     case T_LONG:  // fall through
 735     case T_DOUBLE:
 736 #ifdef _LP64
 737       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 738                                             lo_word_offset_in_bytes), (intptr_t)c-&gt;as_jlong_bits());
 739 #else
 740       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 741                                               lo_word_offset_in_bytes), c-&gt;as_jint_lo_bits());
 742       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 743                                               hi_word_offset_in_bytes), c-&gt;as_jint_hi_bits());
 744 #endif // _LP64
 745       break;
 746 
 747     default:
 748       ShouldNotReachHere();
 749   }
 750 }
 751 
 752 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 753   assert(src-&gt;is_constant(), "should not call otherwise");
 754   assert(dest-&gt;is_address(), "should not call otherwise");
 755   LIR_Const* c = src-&gt;as_constant_ptr();
 756   LIR_Address* addr = dest-&gt;as_address_ptr();
 757 
 758   int null_check_here = code_offset();
 759   switch (type) {
 760     case T_INT:    // fall through
 761     case T_FLOAT:
 762       __ movl(as_Address(addr), c-&gt;as_jint_bits());
 763       break;
 764 
 765     case T_ADDRESS:
 766       __ movptr(as_Address(addr), c-&gt;as_jint_bits());
 767       break;
 768 
 769     case T_OBJECT:  // fall through
 770     case T_ARRAY:
 771       if (c-&gt;as_jobject() == NULL) {
 772         if (UseCompressedOops &amp;&amp; !wide) {
 773           __ movl(as_Address(addr), (int32_t)NULL_WORD);
 774         } else {
 775 #ifdef _LP64
 776           __ xorptr(rscratch1, rscratch1);
 777           null_check_here = code_offset();
 778           __ movptr(as_Address(addr), rscratch1);
 779 #else
 780           __ movptr(as_Address(addr), NULL_WORD);
 781 #endif
 782         }
 783       } else {
 784         if (is_literal_address(addr)) {
 785           ShouldNotReachHere();
 786           __ movoop(as_Address(addr, noreg), c-&gt;as_jobject());
 787         } else {
 788 #ifdef _LP64
 789           __ movoop(rscratch1, c-&gt;as_jobject());
 790           if (UseCompressedOops &amp;&amp; !wide) {
 791             __ encode_heap_oop(rscratch1);
 792             null_check_here = code_offset();
 793             __ movl(as_Address_lo(addr), rscratch1);
 794           } else {
 795             null_check_here = code_offset();
 796             __ movptr(as_Address_lo(addr), rscratch1);
 797           }
 798 #else
 799           __ movoop(as_Address(addr), c-&gt;as_jobject());
 800 #endif
 801         }
 802       }
 803       break;
 804 
 805     case T_LONG:    // fall through
 806     case T_DOUBLE:
 807 #ifdef _LP64
 808       if (is_literal_address(addr)) {
 809         ShouldNotReachHere();
 810         __ movptr(as_Address(addr, r15_thread), (intptr_t)c-&gt;as_jlong_bits());
 811       } else {
 812         __ movptr(r10, (intptr_t)c-&gt;as_jlong_bits());
 813         null_check_here = code_offset();
 814         __ movptr(as_Address_lo(addr), r10);
 815       }
 816 #else
 817       // Always reachable in 32bit so this doesn't produce useless move literal
 818       __ movptr(as_Address_hi(addr), c-&gt;as_jint_hi_bits());
 819       __ movptr(as_Address_lo(addr), c-&gt;as_jint_lo_bits());
 820 #endif // _LP64
 821       break;
 822 
 823     case T_BOOLEAN: // fall through
 824     case T_BYTE:
 825       __ movb(as_Address(addr), c-&gt;as_jint() &amp; 0xFF);
 826       break;
 827 
 828     case T_CHAR:    // fall through
 829     case T_SHORT:
 830       __ movw(as_Address(addr), c-&gt;as_jint() &amp; 0xFFFF);
 831       break;
 832 
 833     default:
 834       ShouldNotReachHere();
 835   };
 836 
 837   if (info != NULL) {
 838     add_debug_info_for_null_check(null_check_here, info);
 839   }
 840 }
 841 
 842 
 843 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 844   assert(src-&gt;is_register(), "should not call otherwise");
 845   assert(dest-&gt;is_register(), "should not call otherwise");
 846 
 847   // move between cpu-registers
 848   if (dest-&gt;is_single_cpu()) {
 849 #ifdef _LP64
 850     if (src-&gt;type() == T_LONG) {
 851       // Can do LONG -&gt; OBJECT
 852       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 853       return;
 854     }
 855 #endif
 856     assert(src-&gt;is_single_cpu(), "must match");
 857     if (src-&gt;type() == T_OBJECT) {
 858       __ verify_oop(src-&gt;as_register());
 859     }
 860     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 861 
 862   } else if (dest-&gt;is_double_cpu()) {
 863 #ifdef _LP64
 864     if (is_reference_type(src-&gt;type())) {
 865       // Surprising to me but we can see move of a long to t_object
 866       __ verify_oop(src-&gt;as_register());
 867       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 868       return;
 869     }
 870 #endif
 871     assert(src-&gt;is_double_cpu(), "must match");
 872     Register f_lo = src-&gt;as_register_lo();
 873     Register f_hi = src-&gt;as_register_hi();
 874     Register t_lo = dest-&gt;as_register_lo();
 875     Register t_hi = dest-&gt;as_register_hi();
 876 #ifdef _LP64
 877     assert(f_hi == f_lo, "must be same");
 878     assert(t_hi == t_lo, "must be same");
 879     move_regs(f_lo, t_lo);
 880 #else
 881     assert(f_lo != f_hi &amp;&amp; t_lo != t_hi, "invalid register allocation");
 882 
 883 
 884     if (f_lo == t_hi &amp;&amp; f_hi == t_lo) {
 885       swap_reg(f_lo, f_hi);
 886     } else if (f_hi == t_lo) {
 887       assert(f_lo != t_hi, "overwriting register");
 888       move_regs(f_hi, t_hi);
 889       move_regs(f_lo, t_lo);
 890     } else {
 891       assert(f_hi != t_lo, "overwriting register");
 892       move_regs(f_lo, t_lo);
 893       move_regs(f_hi, t_hi);
 894     }
 895 #endif // LP64
 896 
 897     // special moves from fpu-register to xmm-register
 898     // necessary for method results
 899   } else if (src-&gt;is_single_xmm() &amp;&amp; !dest-&gt;is_single_xmm()) {
 900     __ movflt(Address(rsp, 0), src-&gt;as_xmm_float_reg());
 901     __ fld_s(Address(rsp, 0));
 902   } else if (src-&gt;is_double_xmm() &amp;&amp; !dest-&gt;is_double_xmm()) {
 903     __ movdbl(Address(rsp, 0), src-&gt;as_xmm_double_reg());
 904     __ fld_d(Address(rsp, 0));
 905   } else if (dest-&gt;is_single_xmm() &amp;&amp; !src-&gt;is_single_xmm()) {
 906     __ fstp_s(Address(rsp, 0));
 907     __ movflt(dest-&gt;as_xmm_float_reg(), Address(rsp, 0));
 908   } else if (dest-&gt;is_double_xmm() &amp;&amp; !src-&gt;is_double_xmm()) {
 909     __ fstp_d(Address(rsp, 0));
 910     __ movdbl(dest-&gt;as_xmm_double_reg(), Address(rsp, 0));
 911 
 912     // move between xmm-registers
 913   } else if (dest-&gt;is_single_xmm()) {
 914     assert(src-&gt;is_single_xmm(), "must match");
 915     __ movflt(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_float_reg());
 916   } else if (dest-&gt;is_double_xmm()) {
 917     assert(src-&gt;is_double_xmm(), "must match");
 918     __ movdbl(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_double_reg());
 919 
 920     // move between fpu-registers (no instruction necessary because of fpu-stack)
 921   } else if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 922     assert(src-&gt;is_single_fpu() || src-&gt;is_double_fpu(), "must match");
 923     assert(src-&gt;fpu() == dest-&gt;fpu(), "currently should be nothing to do");
 924   } else {
 925     ShouldNotReachHere();
 926   }
 927 }
 928 
 929 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 930   assert(src-&gt;is_register(), "should not call otherwise");
 931   assert(dest-&gt;is_stack(), "should not call otherwise");
 932 
 933   if (src-&gt;is_single_cpu()) {
 934     Address dst = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 935     if (is_reference_type(type)) {
 936       __ verify_oop(src-&gt;as_register());
 937       __ movptr (dst, src-&gt;as_register());
 938     } else if (type == T_METADATA) {
 939       __ movptr (dst, src-&gt;as_register());
 940     } else {
 941       __ movl (dst, src-&gt;as_register());
 942     }
 943 
 944   } else if (src-&gt;is_double_cpu()) {
 945     Address dstLO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 946     Address dstHI = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes);
 947     __ movptr (dstLO, src-&gt;as_register_lo());
 948     NOT_LP64(__ movptr (dstHI, src-&gt;as_register_hi()));
 949 
 950   } else if (src-&gt;is_single_xmm()) {
 951     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 952     __ movflt(dst_addr, src-&gt;as_xmm_float_reg());
 953 
 954   } else if (src-&gt;is_double_xmm()) {
 955     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 956     __ movdbl(dst_addr, src-&gt;as_xmm_double_reg());
 957 
 958   } else if (src-&gt;is_single_fpu()) {
 959     assert(src-&gt;fpu_regnr() == 0, "argument must be on TOS");
 960     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 961     if (pop_fpu_stack)     __ fstp_s (dst_addr);
 962     else                   __ fst_s  (dst_addr);
 963 
 964   } else if (src-&gt;is_double_fpu()) {
 965     assert(src-&gt;fpu_regnrLo() == 0, "argument must be on TOS");
 966     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 967     if (pop_fpu_stack)     __ fstp_d (dst_addr);
 968     else                   __ fst_d  (dst_addr);
 969 
 970   } else {
 971     ShouldNotReachHere();
 972   }
 973 }
 974 
 975 
 976 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 977   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 978   PatchingStub* patch = NULL;
 979   Register compressed_src = rscratch1;
 980 
 981   if (is_reference_type(type)) {
 982     __ verify_oop(src-&gt;as_register());
 983 #ifdef _LP64
 984     if (UseCompressedOops &amp;&amp; !wide) {
 985       __ movptr(compressed_src, src-&gt;as_register());
 986       __ encode_heap_oop(compressed_src);
 987       if (patch_code != lir_patch_none) {
 988         info-&gt;oop_map()-&gt;set_narrowoop(compressed_src-&gt;as_VMReg());
 989       }
 990     }
 991 #endif
 992   }
 993 
 994   if (patch_code != lir_patch_none) {
 995     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 996     Address toa = as_Address(to_addr);
 997     assert(toa.disp() != 0, "must have");
 998   }
 999 
1000   int null_check_here = code_offset();
1001   switch (type) {
1002     case T_FLOAT: {
1003       if (src-&gt;is_single_xmm()) {
1004         __ movflt(as_Address(to_addr), src-&gt;as_xmm_float_reg());
1005       } else {
1006         assert(src-&gt;is_single_fpu(), "must be");
1007         assert(src-&gt;fpu_regnr() == 0, "argument must be on TOS");
1008         if (pop_fpu_stack)      __ fstp_s(as_Address(to_addr));
1009         else                    __ fst_s (as_Address(to_addr));
1010       }
1011       break;
1012     }
1013 
1014     case T_DOUBLE: {
1015       if (src-&gt;is_double_xmm()) {
1016         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1017       } else {
1018         assert(src-&gt;is_double_fpu(), "must be");
1019         assert(src-&gt;fpu_regnrLo() == 0, "argument must be on TOS");
1020         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1021         else                    __ fst_d (as_Address(to_addr));
1022       }
1023       break;
1024     }
1025 
1026     case T_ARRAY:   // fall through
1027     case T_OBJECT:  // fall through
1028       if (UseCompressedOops &amp;&amp; !wide) {
1029         __ movl(as_Address(to_addr), compressed_src);
1030       } else {
1031         __ movptr(as_Address(to_addr), src-&gt;as_register());
1032       }
1033       break;
1034     case T_METADATA:
1035       // We get here to store a method pointer to the stack to pass to
1036       // a dtrace runtime call. This can't work on 64 bit with
1037       // compressed klass ptrs: T_METADATA can be a compressed klass
1038       // ptr or a 64 bit method pointer.
1039       LP64_ONLY(ShouldNotReachHere());
1040       __ movptr(as_Address(to_addr), src-&gt;as_register());
1041       break;
1042     case T_ADDRESS:
1043       __ movptr(as_Address(to_addr), src-&gt;as_register());
1044       break;
1045     case T_INT:
1046       __ movl(as_Address(to_addr), src-&gt;as_register());
1047       break;
1048 
1049     case T_LONG: {
1050       Register from_lo = src-&gt;as_register_lo();
1051       Register from_hi = src-&gt;as_register_hi();
1052 #ifdef _LP64
1053       __ movptr(as_Address_lo(to_addr), from_lo);
1054 #else
1055       Register base = to_addr-&gt;base()-&gt;as_register();
1056       Register index = noreg;
1057       if (to_addr-&gt;index()-&gt;is_register()) {
1058         index = to_addr-&gt;index()-&gt;as_register();
1059       }
1060       if (base == from_lo || index == from_lo) {
1061         assert(base != from_hi, "can't be");
1062         assert(index == noreg || (index != base &amp;&amp; index != from_hi), "can't handle this");
1063         __ movl(as_Address_hi(to_addr), from_hi);
1064         if (patch != NULL) {
1065           patching_epilog(patch, lir_patch_high, base, info);
1066           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1067           patch_code = lir_patch_low;
1068         }
1069         __ movl(as_Address_lo(to_addr), from_lo);
1070       } else {
1071         assert(index == noreg || (index != base &amp;&amp; index != from_lo), "can't handle this");
1072         __ movl(as_Address_lo(to_addr), from_lo);
1073         if (patch != NULL) {
1074           patching_epilog(patch, lir_patch_low, base, info);
1075           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1076           patch_code = lir_patch_high;
1077         }
1078         __ movl(as_Address_hi(to_addr), from_hi);
1079       }
1080 #endif // _LP64
1081       break;
1082     }
1083 
1084     case T_BYTE:    // fall through
1085     case T_BOOLEAN: {
1086       Register src_reg = src-&gt;as_register();
1087       Address dst_addr = as_Address(to_addr);
1088       assert(VM_Version::is_P6() || src_reg-&gt;has_byte_register(), "must use byte registers if not P6");
1089       __ movb(dst_addr, src_reg);
1090       break;
1091     }
1092 
1093     case T_CHAR:    // fall through
1094     case T_SHORT:
1095       __ movw(as_Address(to_addr), src-&gt;as_register());
1096       break;
1097 
1098     default:
1099       ShouldNotReachHere();
1100   }
1101   if (info != NULL) {
1102     add_debug_info_for_null_check(null_check_here, info);
1103   }
1104 
1105   if (patch_code != lir_patch_none) {
1106     patching_epilog(patch, patch_code, to_addr-&gt;base()-&gt;as_register(), info);
1107   }
1108 }
1109 
1110 
1111 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
1112   assert(src-&gt;is_stack(), "should not call otherwise");
1113   assert(dest-&gt;is_register(), "should not call otherwise");
1114 
1115   if (dest-&gt;is_single_cpu()) {
1116     if (is_reference_type(type)) {
1117       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1118       __ verify_oop(dest-&gt;as_register());
1119     } else if (type == T_METADATA) {
1120       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1121     } else {
1122       __ movl(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1123     }
1124 
1125   } else if (dest-&gt;is_double_cpu()) {
1126     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
1127     Address src_addr_HI = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes);
1128     __ movptr(dest-&gt;as_register_lo(), src_addr_LO);
1129     NOT_LP64(__ movptr(dest-&gt;as_register_hi(), src_addr_HI));
1130 
1131   } else if (dest-&gt;is_single_xmm()) {
1132     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1133     __ movflt(dest-&gt;as_xmm_float_reg(), src_addr);
1134 
1135   } else if (dest-&gt;is_double_xmm()) {
1136     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1137     __ movdbl(dest-&gt;as_xmm_double_reg(), src_addr);
1138 
1139   } else if (dest-&gt;is_single_fpu()) {
1140     assert(dest-&gt;fpu_regnr() == 0, "dest must be TOS");
1141     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1142     __ fld_s(src_addr);
1143 
1144   } else if (dest-&gt;is_double_fpu()) {
1145     assert(dest-&gt;fpu_regnrLo() == 0, "dest must be TOS");
1146     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1147     __ fld_d(src_addr);
1148 
1149   } else {
1150     ShouldNotReachHere();
1151   }
1152 }
1153 
1154 
1155 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
1156   if (src-&gt;is_single_stack()) {
1157     if (is_reference_type(type)) {
1158       __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1159       __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1160     } else {
1161 #ifndef _LP64
1162       __ pushl(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1163       __ popl (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1164 #else
1165       //no pushl on 64bits
1166       __ movl(rscratch1, frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1167       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), rscratch1);
1168 #endif
1169     }
1170 
1171   } else if (src-&gt;is_double_stack()) {
1172 #ifdef _LP64
1173     __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix()));
1174     __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
1175 #else
1176     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 0));
1177     // push and pop the part at src + wordSize, adding wordSize for the previous push
1178     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 2 * wordSize));
1179     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 2 * wordSize));
1180     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 0));
1181 #endif // _LP64
1182 
1183   } else {
1184     ShouldNotReachHere();
1185   }
1186 }
1187 
1188 
1189 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
1190   assert(src-&gt;is_address(), "should not call otherwise");
1191   assert(dest-&gt;is_register(), "should not call otherwise");
1192 
1193   LIR_Address* addr = src-&gt;as_address_ptr();
1194   Address from_addr = as_Address(addr);
1195 
1196   if (addr-&gt;base()-&gt;type() == T_OBJECT) {
1197     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
1198   }
1199 
1200   switch (type) {
1201     case T_BOOLEAN: // fall through
1202     case T_BYTE:    // fall through
1203     case T_CHAR:    // fall through
1204     case T_SHORT:
1205       if (!VM_Version::is_P6() &amp;&amp; !from_addr.uses(dest-&gt;as_register())) {
1206         // on pre P6 processors we may get partial register stalls
1207         // so blow away the value of to_rinfo before loading a
1208         // partial word into it.  Do it here so that it precedes
1209         // the potential patch point below.
1210         __ xorptr(dest-&gt;as_register(), dest-&gt;as_register());
1211       }
1212       break;
1213    default:
1214      break;
1215   }
1216 
1217   PatchingStub* patch = NULL;
1218   if (patch_code != lir_patch_none) {
1219     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1220     assert(from_addr.disp() != 0, "must have");
1221   }
1222   if (info != NULL) {
1223     add_debug_info_for_null_check_here(info);
1224   }
1225 
1226   switch (type) {
1227     case T_FLOAT: {
1228       if (dest-&gt;is_single_xmm()) {
1229         __ movflt(dest-&gt;as_xmm_float_reg(), from_addr);
1230       } else {
1231         assert(dest-&gt;is_single_fpu(), "must be");
1232         assert(dest-&gt;fpu_regnr() == 0, "dest must be TOS");
1233         __ fld_s(from_addr);
1234       }
1235       break;
1236     }
1237 
1238     case T_DOUBLE: {
1239       if (dest-&gt;is_double_xmm()) {
1240         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1241       } else {
1242         assert(dest-&gt;is_double_fpu(), "must be");
1243         assert(dest-&gt;fpu_regnrLo() == 0, "dest must be TOS");
1244         __ fld_d(from_addr);
1245       }
1246       break;
1247     }
1248 
1249     case T_OBJECT:  // fall through
1250     case T_ARRAY:   // fall through
1251       if (UseCompressedOops &amp;&amp; !wide) {
1252         __ movl(dest-&gt;as_register(), from_addr);
1253       } else {
1254         __ movptr(dest-&gt;as_register(), from_addr);
1255       }
1256       break;
1257 
1258     case T_ADDRESS:
1259       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1260         __ movl(dest-&gt;as_register(), from_addr);
1261       } else {
1262         __ movptr(dest-&gt;as_register(), from_addr);
1263       }
1264       break;
1265     case T_INT:
1266       __ movl(dest-&gt;as_register(), from_addr);
1267       break;
1268 
1269     case T_LONG: {
1270       Register to_lo = dest-&gt;as_register_lo();
1271       Register to_hi = dest-&gt;as_register_hi();
1272 #ifdef _LP64
1273       __ movptr(to_lo, as_Address_lo(addr));
1274 #else
1275       Register base = addr-&gt;base()-&gt;as_register();
1276       Register index = noreg;
1277       if (addr-&gt;index()-&gt;is_register()) {
1278         index = addr-&gt;index()-&gt;as_register();
1279       }
1280       if ((base == to_lo &amp;&amp; index == to_hi) ||
1281           (base == to_hi &amp;&amp; index == to_lo)) {
1282         // addresses with 2 registers are only formed as a result of
1283         // array access so this code will never have to deal with
1284         // patches or null checks.
1285         assert(info == NULL &amp;&amp; patch == NULL, "must be");
1286         __ lea(to_hi, as_Address(addr));
1287         __ movl(to_lo, Address(to_hi, 0));
1288         __ movl(to_hi, Address(to_hi, BytesPerWord));
1289       } else if (base == to_lo || index == to_lo) {
1290         assert(base != to_hi, "can't be");
1291         assert(index == noreg || (index != base &amp;&amp; index != to_hi), "can't handle this");
1292         __ movl(to_hi, as_Address_hi(addr));
1293         if (patch != NULL) {
1294           patching_epilog(patch, lir_patch_high, base, info);
1295           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1296           patch_code = lir_patch_low;
1297         }
1298         __ movl(to_lo, as_Address_lo(addr));
1299       } else {
1300         assert(index == noreg || (index != base &amp;&amp; index != to_lo), "can't handle this");
1301         __ movl(to_lo, as_Address_lo(addr));
1302         if (patch != NULL) {
1303           patching_epilog(patch, lir_patch_low, base, info);
1304           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1305           patch_code = lir_patch_high;
1306         }
1307         __ movl(to_hi, as_Address_hi(addr));
1308       }
1309 #endif // _LP64
1310       break;
1311     }
1312 
1313     case T_BOOLEAN: // fall through
1314     case T_BYTE: {
1315       Register dest_reg = dest-&gt;as_register();
1316       assert(VM_Version::is_P6() || dest_reg-&gt;has_byte_register(), "must use byte registers if not P6");
1317       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1318         __ movsbl(dest_reg, from_addr);
1319       } else {
1320         __ movb(dest_reg, from_addr);
1321         __ shll(dest_reg, 24);
1322         __ sarl(dest_reg, 24);
1323       }
1324       break;
1325     }
1326 
1327     case T_CHAR: {
1328       Register dest_reg = dest-&gt;as_register();
1329       assert(VM_Version::is_P6() || dest_reg-&gt;has_byte_register(), "must use byte registers if not P6");
1330       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1331         __ movzwl(dest_reg, from_addr);
1332       } else {
1333         __ movw(dest_reg, from_addr);
1334       }
1335       break;
1336     }
1337 
1338     case T_SHORT: {
1339       Register dest_reg = dest-&gt;as_register();
1340       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1341         __ movswl(dest_reg, from_addr);
1342       } else {
1343         __ movw(dest_reg, from_addr);
1344         __ shll(dest_reg, 16);
1345         __ sarl(dest_reg, 16);
1346       }
1347       break;
1348     }
1349 
1350     default:
1351       ShouldNotReachHere();
1352   }
1353 
1354   if (patch != NULL) {
1355     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
1356   }
1357 
1358   if (is_reference_type(type)) {
1359 #ifdef _LP64
1360     if (UseCompressedOops &amp;&amp; !wide) {
1361       __ decode_heap_oop(dest-&gt;as_register());
1362     }
1363 #endif
1364 
1365     // Load barrier has not yet been applied, so ZGC can't verify the oop here
1366     if (!UseZGC) {
1367       __ verify_oop(dest-&gt;as_register());
1368     }
1369   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1370 #ifdef _LP64
1371     if (UseCompressedClassPointers) {
1372       __ decode_klass_not_null(dest-&gt;as_register());
1373     }
1374 #endif
1375   }
1376 }
1377 
1378 
1379 NEEDS_CLEANUP; // This could be static?
1380 Address::ScaleFactor LIR_Assembler::array_element_size(BasicType type) const {
1381   int elem_size = type2aelembytes(type);
1382   switch (elem_size) {
1383     case 1: return Address::times_1;
1384     case 2: return Address::times_2;
1385     case 4: return Address::times_4;
1386     case 8: return Address::times_8;
1387   }
1388   ShouldNotReachHere();
1389   return Address::no_scale;
1390 }
1391 
1392 
1393 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1394   switch (op-&gt;code()) {
1395     case lir_idiv:
1396     case lir_irem:
1397       arithmetic_idiv(op-&gt;code(),
1398                       op-&gt;in_opr1(),
1399                       op-&gt;in_opr2(),
1400                       op-&gt;in_opr3(),
1401                       op-&gt;result_opr(),
1402                       op-&gt;info());
1403       break;
1404     case lir_fmad:
1405       __ fmad(op-&gt;result_opr()-&gt;as_xmm_double_reg(),
1406               op-&gt;in_opr1()-&gt;as_xmm_double_reg(),
1407               op-&gt;in_opr2()-&gt;as_xmm_double_reg(),
1408               op-&gt;in_opr3()-&gt;as_xmm_double_reg());
1409       break;
1410     case lir_fmaf:
1411       __ fmaf(op-&gt;result_opr()-&gt;as_xmm_float_reg(),
1412               op-&gt;in_opr1()-&gt;as_xmm_float_reg(),
1413               op-&gt;in_opr2()-&gt;as_xmm_float_reg(),
1414               op-&gt;in_opr3()-&gt;as_xmm_float_reg());
1415       break;
1416     default:      ShouldNotReachHere(); break;
1417   }
1418 }
1419 
1420 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1421 #ifdef ASSERT
1422   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), "wrong label");
1423   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1424   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1425 #endif
1426 
1427   if (op-&gt;cond() == lir_cond_always) {
1428     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1429     __ jmp (*(op-&gt;label()));
1430   } else {
1431     Assembler::Condition acond = Assembler::zero;
1432     if (op-&gt;code() == lir_cond_float_branch) {
1433       assert(op-&gt;ublock() != NULL, "must have unordered successor");
1434       __ jcc(Assembler::parity, *(op-&gt;ublock()-&gt;label()));
1435       switch(op-&gt;cond()) {
1436         case lir_cond_equal:        acond = Assembler::equal;      break;
1437         case lir_cond_notEqual:     acond = Assembler::notEqual;   break;
1438         case lir_cond_less:         acond = Assembler::below;      break;
1439         case lir_cond_lessEqual:    acond = Assembler::belowEqual; break;
1440         case lir_cond_greaterEqual: acond = Assembler::aboveEqual; break;
1441         case lir_cond_greater:      acond = Assembler::above;      break;
1442         default:                         ShouldNotReachHere();
1443       }
1444     } else {
1445       switch (op-&gt;cond()) {
1446         case lir_cond_equal:        acond = Assembler::equal;       break;
1447         case lir_cond_notEqual:     acond = Assembler::notEqual;    break;
1448         case lir_cond_less:         acond = Assembler::less;        break;
1449         case lir_cond_lessEqual:    acond = Assembler::lessEqual;   break;
1450         case lir_cond_greaterEqual: acond = Assembler::greaterEqual;break;
1451         case lir_cond_greater:      acond = Assembler::greater;     break;
1452         case lir_cond_belowEqual:   acond = Assembler::belowEqual;  break;
1453         case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;  break;
1454         default:                         ShouldNotReachHere();
1455       }
1456     }
1457     __ jcc(acond,*(op-&gt;label()));
1458   }
1459 }
1460 
1461 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
1462   LIR_Opr src  = op-&gt;in_opr();
1463   LIR_Opr dest = op-&gt;result_opr();
1464 
1465   switch (op-&gt;bytecode()) {
1466     case Bytecodes::_i2l:
1467 #ifdef _LP64
1468       __ movl2ptr(dest-&gt;as_register_lo(), src-&gt;as_register());
1469 #else
1470       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
1471       move_regs(src-&gt;as_register(), dest-&gt;as_register_hi());
1472       __ sarl(dest-&gt;as_register_hi(), 31);
1473 #endif // LP64
1474       break;
1475 
1476     case Bytecodes::_l2i:
1477 #ifdef _LP64
1478       __ movl(dest-&gt;as_register(), src-&gt;as_register_lo());
1479 #else
1480       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
1481 #endif
1482       break;
1483 
1484     case Bytecodes::_i2b:
1485       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1486       __ sign_extend_byte(dest-&gt;as_register());
1487       break;
1488 
1489     case Bytecodes::_i2c:
1490       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1491       __ andl(dest-&gt;as_register(), 0xFFFF);
1492       break;
1493 
1494     case Bytecodes::_i2s:
1495       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1496       __ sign_extend_short(dest-&gt;as_register());
1497       break;
1498 
1499 
1500     case Bytecodes::_f2d:
1501     case Bytecodes::_d2f:
1502       if (dest-&gt;is_single_xmm()) {
1503         __ cvtsd2ss(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_double_reg());
1504       } else if (dest-&gt;is_double_xmm()) {
1505         __ cvtss2sd(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_float_reg());
1506       } else {
1507         assert(src-&gt;fpu() == dest-&gt;fpu(), "register must be equal");
1508         // do nothing (float result is rounded later through spilling)
1509       }
1510       break;
1511 
1512     case Bytecodes::_i2f:
1513     case Bytecodes::_i2d:
1514       if (dest-&gt;is_single_xmm()) {
1515         __ cvtsi2ssl(dest-&gt;as_xmm_float_reg(), src-&gt;as_register());
1516       } else if (dest-&gt;is_double_xmm()) {
1517         __ cvtsi2sdl(dest-&gt;as_xmm_double_reg(), src-&gt;as_register());
1518       } else {
1519         assert(dest-&gt;fpu() == 0, "result must be on TOS");
1520         __ movl(Address(rsp, 0), src-&gt;as_register());
1521         __ fild_s(Address(rsp, 0));
1522       }
1523       break;
1524 
1525     case Bytecodes::_f2i:
1526     case Bytecodes::_d2i:
1527       if (src-&gt;is_single_xmm()) {
1528         __ cvttss2sil(dest-&gt;as_register(), src-&gt;as_xmm_float_reg());
1529       } else if (src-&gt;is_double_xmm()) {
1530         __ cvttsd2sil(dest-&gt;as_register(), src-&gt;as_xmm_double_reg());
1531       } else {
1532         assert(src-&gt;fpu() == 0, "input must be on TOS");
1533         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_trunc()));
1534         __ fist_s(Address(rsp, 0));
1535         __ movl(dest-&gt;as_register(), Address(rsp, 0));
1536         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
1537       }
1538 
1539       // IA32 conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
1540       assert(op-&gt;stub() != NULL, "stub required");
1541       __ cmpl(dest-&gt;as_register(), 0x80000000);
1542       __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry());
1543       __ bind(*op-&gt;stub()-&gt;continuation());
1544       break;
1545 
1546     case Bytecodes::_l2f:
1547     case Bytecodes::_l2d:
1548       assert(!dest-&gt;is_xmm_register(), "result in xmm register not supported (no SSE instruction present)");
1549       assert(dest-&gt;fpu() == 0, "result must be on TOS");
1550 
1551       __ movptr(Address(rsp, 0),            src-&gt;as_register_lo());
1552       NOT_LP64(__ movl(Address(rsp, BytesPerWord), src-&gt;as_register_hi()));
1553       __ fild_d(Address(rsp, 0));
1554       // float result is rounded later through spilling
1555       break;
1556 
1557     case Bytecodes::_f2l:
1558     case Bytecodes::_d2l:
1559       assert(!src-&gt;is_xmm_register(), "input in xmm register not supported (no SSE instruction present)");
1560       assert(src-&gt;fpu() == 0, "input must be on TOS");
1561       assert(dest == FrameMap::long0_opr, "runtime stub places result in these registers");
1562 
1563       // instruction sequence too long to inline it here
1564       {
1565         __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::fpu2long_stub_id)));
1566       }
1567       break;
1568 
1569     default: ShouldNotReachHere();
1570   }
1571 }
1572 
1573 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1574   if (op-&gt;init_check()) {
1575     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1576     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1577                     InstanceKlass::init_state_offset()),
1578                     InstanceKlass::fully_initialized);
1579     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1580   }
1581   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1582                      op-&gt;tmp1()-&gt;as_register(),
1583                      op-&gt;tmp2()-&gt;as_register(),
1584                      op-&gt;header_size(),
1585                      op-&gt;object_size(),
1586                      op-&gt;klass()-&gt;as_register(),
1587                      *op-&gt;stub()-&gt;entry());
1588   __ bind(*op-&gt;stub()-&gt;continuation());
1589 }
1590 
1591 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1592   Register len =  op-&gt;len()-&gt;as_register();
1593   LP64_ONLY( __ movslq(len, len); )
1594 
1595   if (UseSlowPath ||
1596       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1597       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1598     __ jmp(*op-&gt;stub()-&gt;entry());
1599   } else {
1600     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1601     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1602     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1603     if (len == tmp1) {
1604       tmp1 = tmp3;
1605     } else if (len == tmp2) {
1606       tmp2 = tmp3;
1607     } else if (len == tmp3) {
1608       // everything is ok
1609     } else {
1610       __ mov(tmp3, len);
1611     }
1612     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1613                       len,
1614                       tmp1,
1615                       tmp2,
1616                       arrayOopDesc::header_size(op-&gt;type()),
1617                       array_element_size(op-&gt;type()),
1618                       op-&gt;klass()-&gt;as_register(),
1619                       *op-&gt;stub()-&gt;entry());
1620   }
1621   __ bind(*op-&gt;stub()-&gt;continuation());
1622 }
1623 
1624 void LIR_Assembler::type_profile_helper(Register mdo,
1625                                         ciMethodData *md, ciProfileData *data,
1626                                         Register recv, Label* update_done) {
1627   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1628     Label next_test;
1629     // See if the receiver is receiver[n].
1630     __ cmpptr(recv, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1631     __ jccb(Assembler::notEqual, next_test);
1632     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));
1633     __ addptr(data_addr, DataLayout::counter_increment);
1634     __ jmp(*update_done);
1635     __ bind(next_test);
1636   }
1637 
1638   // Didn't find receiver; find next empty slot and fill it in
1639   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1640     Label next_test;
1641     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)));
1642     __ cmpptr(recv_addr, (intptr_t)NULL_WORD);
1643     __ jccb(Assembler::notEqual, next_test);
1644     __ movptr(recv_addr, recv);
1645     __ movptr(Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i))), DataLayout::counter_increment);
1646     __ jmp(*update_done);
1647     __ bind(next_test);
1648   }
1649 }
1650 
1651 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
1652   // we always need a stub for the failure case.
1653   CodeStub* stub = op-&gt;stub();
1654   Register obj = op-&gt;object()-&gt;as_register();
1655   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1656   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1657   Register dst = op-&gt;result_opr()-&gt;as_register();
1658   ciKlass* k = op-&gt;klass();
1659   Register Rtmp1 = noreg;
1660 
1661   // check if it needs to be profiled
1662   ciMethodData* md = NULL;
1663   ciProfileData* data = NULL;
1664 
1665   if (op-&gt;should_profile()) {
1666     ciMethod* method = op-&gt;profiled_method();
1667     assert(method != NULL, "Should have method");
1668     int bci = op-&gt;profiled_bci();
1669     md = method-&gt;method_data_or_null();
1670     assert(md != NULL, "Sanity");
1671     data = md-&gt;bci_to_data(bci);
1672     assert(data != NULL,                "need data for type check");
1673     assert(data-&gt;is_ReceiverTypeData(), "need ReceiverTypeData for type check");
1674   }
1675   Label profile_cast_success, profile_cast_failure;
1676   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
1677   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
1678 
1679   if (obj == k_RInfo) {
1680     k_RInfo = dst;
1681   } else if (obj == klass_RInfo) {
1682     klass_RInfo = dst;
1683   }
1684   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1685     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1686   } else {
1687     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1688     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1689   }
1690 
1691   assert_different_registers(obj, k_RInfo, klass_RInfo);
1692 
1693   __ cmpptr(obj, (int32_t)NULL_WORD);
1694   if (op-&gt;should_profile()) {
1695     Label not_null;
1696     __ jccb(Assembler::notEqual, not_null);
1697     // Object is null; update MDO and exit
1698     Register mdo  = klass_RInfo;
1699     __ mov_metadata(mdo, md-&gt;constant_encoding());
1700     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));
1701     int header_bits = BitData::null_seen_byte_constant();
1702     __ orb(data_addr, header_bits);
1703     __ jmp(*obj_is_null);
1704     __ bind(not_null);
1705   } else {
1706     __ jcc(Assembler::equal, *obj_is_null);
1707   }
1708 
1709   if (!k-&gt;is_loaded()) {
1710     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1711   } else {
1712 #ifdef _LP64
1713     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1714 #endif // _LP64
1715   }
1716   __ verify_oop(obj);
1717 
1718   if (op-&gt;fast_check()) {
1719     // get object class
1720     // not a safepoint as obj null check happens earlier
1721 #ifdef _LP64
1722     if (UseCompressedClassPointers) {
1723       __ load_klass(Rtmp1, obj);
1724       __ cmpptr(k_RInfo, Rtmp1);
1725     } else {
1726       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
1727     }
1728 #else
1729     if (k-&gt;is_loaded()) {
1730       __ cmpklass(Address(obj, oopDesc::klass_offset_in_bytes()), k-&gt;constant_encoding());
1731     } else {
1732       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
1733     }
1734 #endif
1735     __ jcc(Assembler::notEqual, *failure_target);
1736     // successful cast, fall through to profile or jump
1737   } else {
1738     // get object class
1739     // not a safepoint as obj null check happens earlier
1740     __ load_klass(klass_RInfo, obj);
1741     if (k-&gt;is_loaded()) {
1742       // See if we get an immediate positive hit
1743 #ifdef _LP64
1744       __ cmpptr(k_RInfo, Address(klass_RInfo, k-&gt;super_check_offset()));
1745 #else
1746       __ cmpklass(Address(klass_RInfo, k-&gt;super_check_offset()), k-&gt;constant_encoding());
1747 #endif // _LP64
1748       if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k-&gt;super_check_offset()) {
1749         __ jcc(Assembler::notEqual, *failure_target);
1750         // successful cast, fall through to profile or jump
1751       } else {
1752         // See if we get an immediate positive hit
1753         __ jcc(Assembler::equal, *success_target);
1754         // check for self
1755 #ifdef _LP64
1756         __ cmpptr(klass_RInfo, k_RInfo);
1757 #else
1758         __ cmpklass(klass_RInfo, k-&gt;constant_encoding());
1759 #endif // _LP64
1760         __ jcc(Assembler::equal, *success_target);
1761 
1762         __ push(klass_RInfo);
1763 #ifdef _LP64
1764         __ push(k_RInfo);
1765 #else
1766         __ pushklass(k-&gt;constant_encoding());
1767 #endif // _LP64
1768         __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1769         __ pop(klass_RInfo);
1770         __ pop(klass_RInfo);
1771         // result is a boolean
1772         __ cmpl(klass_RInfo, 0);
1773         __ jcc(Assembler::equal, *failure_target);
1774         // successful cast, fall through to profile or jump
1775       }
1776     } else {
1777       // perform the fast part of the checking logic
1778       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1779       // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1780       __ push(klass_RInfo);
1781       __ push(k_RInfo);
1782       __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1783       __ pop(klass_RInfo);
1784       __ pop(k_RInfo);
1785       // result is a boolean
1786       __ cmpl(k_RInfo, 0);
1787       __ jcc(Assembler::equal, *failure_target);
1788       // successful cast, fall through to profile or jump
1789     }
1790   }
1791   if (op-&gt;should_profile()) {
1792     Register mdo  = klass_RInfo, recv = k_RInfo;
1793     __ bind(profile_cast_success);
1794     __ mov_metadata(mdo, md-&gt;constant_encoding());
1795     __ load_klass(recv, obj);
1796     type_profile_helper(mdo, md, data, recv, success);
1797     __ jmp(*success);
1798 
1799     __ bind(profile_cast_failure);
1800     __ mov_metadata(mdo, md-&gt;constant_encoding());
1801     Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1802     __ subptr(counter_addr, DataLayout::counter_increment);
1803     __ jmp(*failure);
1804   }
1805   __ jmp(*success);
1806 }
1807 
1808 
1809 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1810   LIR_Code code = op-&gt;code();
1811   if (code == lir_store_check) {
1812     Register value = op-&gt;object()-&gt;as_register();
1813     Register array = op-&gt;array()-&gt;as_register();
1814     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1815     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1816     Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1817 
1818     CodeStub* stub = op-&gt;stub();
1819 
1820     // check if it needs to be profiled
1821     ciMethodData* md = NULL;
1822     ciProfileData* data = NULL;
1823 
1824     if (op-&gt;should_profile()) {
1825       ciMethod* method = op-&gt;profiled_method();
1826       assert(method != NULL, "Should have method");
1827       int bci = op-&gt;profiled_bci();
1828       md = method-&gt;method_data_or_null();
1829       assert(md != NULL, "Sanity");
1830       data = md-&gt;bci_to_data(bci);
1831       assert(data != NULL,                "need data for type check");
1832       assert(data-&gt;is_ReceiverTypeData(), "need ReceiverTypeData for type check");
1833     }
1834     Label profile_cast_success, profile_cast_failure, done;
1835     Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1836     Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
1837 
1838     __ cmpptr(value, (int32_t)NULL_WORD);
1839     if (op-&gt;should_profile()) {
1840       Label not_null;
1841       __ jccb(Assembler::notEqual, not_null);
1842       // Object is null; update MDO and exit
1843       Register mdo  = klass_RInfo;
1844       __ mov_metadata(mdo, md-&gt;constant_encoding());
1845       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));
1846       int header_bits = BitData::null_seen_byte_constant();
1847       __ orb(data_addr, header_bits);
1848       __ jmp(done);
1849       __ bind(not_null);
1850     } else {
1851       __ jcc(Assembler::equal, done);
1852     }
1853 
1854     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1855     __ load_klass(k_RInfo, array);
1856     __ load_klass(klass_RInfo, value);
1857 
1858     // get instance klass (it's already uncompressed)
1859     __ movptr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1860     // perform the fast part of the checking logic
1861     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1862     // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1863     __ push(klass_RInfo);
1864     __ push(k_RInfo);
1865     __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1866     __ pop(klass_RInfo);
1867     __ pop(k_RInfo);
1868     // result is a boolean
1869     __ cmpl(k_RInfo, 0);
1870     __ jcc(Assembler::equal, *failure_target);
1871     // fall through to the success case
1872 
1873     if (op-&gt;should_profile()) {
1874       Register mdo  = klass_RInfo, recv = k_RInfo;
1875       __ bind(profile_cast_success);
1876       __ mov_metadata(mdo, md-&gt;constant_encoding());
1877       __ load_klass(recv, value);
1878       type_profile_helper(mdo, md, data, recv, &amp;done);
1879       __ jmpb(done);
1880 
1881       __ bind(profile_cast_failure);
1882       __ mov_metadata(mdo, md-&gt;constant_encoding());
1883       Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1884       __ subptr(counter_addr, DataLayout::counter_increment);
1885       __ jmp(*stub-&gt;entry());
1886     }
1887 
1888     __ bind(done);
1889   } else
1890     if (code == lir_checkcast) {
1891       Register obj = op-&gt;object()-&gt;as_register();
1892       Register dst = op-&gt;result_opr()-&gt;as_register();
1893       Label success;
1894       emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
1895       __ bind(success);
1896       if (dst != obj) {
1897         __ mov(dst, obj);
1898       }
1899     } else
1900       if (code == lir_instanceof) {
1901         Register obj = op-&gt;object()-&gt;as_register();
1902         Register dst = op-&gt;result_opr()-&gt;as_register();
1903         Label success, failure, done;
1904         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1905         __ bind(failure);
1906         __ xorptr(dst, dst);
1907         __ jmpb(done);
1908         __ bind(success);
1909         __ movptr(dst, 1);
1910         __ bind(done);
1911       } else {
1912         ShouldNotReachHere();
1913       }
1914 
1915 }
1916 
1917 
1918 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1919   if (LP64_ONLY(false &amp;&amp;) op-&gt;code() == lir_cas_long &amp;&amp; VM_Version::supports_cx8()) {
1920     assert(op-&gt;cmp_value()-&gt;as_register_lo() == rax, "wrong register");
1921     assert(op-&gt;cmp_value()-&gt;as_register_hi() == rdx, "wrong register");
1922     assert(op-&gt;new_value()-&gt;as_register_lo() == rbx, "wrong register");
1923     assert(op-&gt;new_value()-&gt;as_register_hi() == rcx, "wrong register");
1924     Register addr = op-&gt;addr()-&gt;as_register();
1925     __ lock();
1926     NOT_LP64(__ cmpxchg8(Address(addr, 0)));
1927 
1928   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj ) {
1929     NOT_LP64(assert(op-&gt;addr()-&gt;is_single_cpu(), "must be single");)
1930     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
1931     Register newval = op-&gt;new_value()-&gt;as_register();
1932     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1933     assert(cmpval == rax, "wrong register");
1934     assert(newval != NULL, "new val must be register");
1935     assert(cmpval != newval, "cmp and new values must be in different registers");
1936     assert(cmpval != addr, "cmp and addr must be in different registers");
1937     assert(newval != addr, "new value and addr must be in different registers");
1938 
1939     if ( op-&gt;code() == lir_cas_obj) {
1940 #ifdef _LP64
1941       if (UseCompressedOops) {
1942         __ encode_heap_oop(cmpval);
1943         __ mov(rscratch1, newval);
1944         __ encode_heap_oop(rscratch1);
1945         __ lock();
1946         // cmpval (rax) is implicitly used by this instruction
1947         __ cmpxchgl(rscratch1, Address(addr, 0));
1948       } else
1949 #endif
1950       {
1951         __ lock();
1952         __ cmpxchgptr(newval, Address(addr, 0));
1953       }
1954     } else {
1955       assert(op-&gt;code() == lir_cas_int, "lir_cas_int expected");
1956       __ lock();
1957       __ cmpxchgl(newval, Address(addr, 0));
1958     }
1959 #ifdef _LP64
1960   } else if (op-&gt;code() == lir_cas_long) {
1961     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
1962     Register newval = op-&gt;new_value()-&gt;as_register_lo();
1963     Register cmpval = op-&gt;cmp_value()-&gt;as_register_lo();
1964     assert(cmpval == rax, "wrong register");
1965     assert(newval != NULL, "new val must be register");
1966     assert(cmpval != newval, "cmp and new values must be in different registers");
1967     assert(cmpval != addr, "cmp and addr must be in different registers");
1968     assert(newval != addr, "new value and addr must be in different registers");
1969     __ lock();
1970     __ cmpxchgq(newval, Address(addr, 0));
1971 #endif // _LP64
1972   } else {
1973     Unimplemented();
1974   }
1975 }
1976 
1977 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1978   Assembler::Condition acond, ncond;
1979   switch (condition) {
1980     case lir_cond_equal:        acond = Assembler::equal;        ncond = Assembler::notEqual;     break;
1981     case lir_cond_notEqual:     acond = Assembler::notEqual;     ncond = Assembler::equal;        break;
1982     case lir_cond_less:         acond = Assembler::less;         ncond = Assembler::greaterEqual; break;
1983     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    ncond = Assembler::greater;      break;
1984     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; ncond = Assembler::less;         break;
1985     case lir_cond_greater:      acond = Assembler::greater;      ncond = Assembler::lessEqual;    break;
1986     case lir_cond_belowEqual:   acond = Assembler::belowEqual;   ncond = Assembler::above;        break;
1987     case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;   ncond = Assembler::below;        break;
1988     default:                    acond = Assembler::equal;        ncond = Assembler::notEqual;
1989                                 ShouldNotReachHere();
1990   }
1991 
1992   if (opr1-&gt;is_cpu_register()) {
1993     reg2reg(opr1, result);
1994   } else if (opr1-&gt;is_stack()) {
1995     stack2reg(opr1, result, result-&gt;type());
1996   } else if (opr1-&gt;is_constant()) {
1997     const2reg(opr1, result, lir_patch_none, NULL);
1998   } else {
1999     ShouldNotReachHere();
2000   }
2001 
2002   if (VM_Version::supports_cmov() &amp;&amp; !opr2-&gt;is_constant()) {
2003     // optimized version that does not require a branch
2004     if (opr2-&gt;is_single_cpu()) {
2005       assert(opr2-&gt;cpu_regnr() != result-&gt;cpu_regnr(), "opr2 already overwritten by previous move");
2006       __ cmov(ncond, result-&gt;as_register(), opr2-&gt;as_register());
2007     } else if (opr2-&gt;is_double_cpu()) {
2008       assert(opr2-&gt;cpu_regnrLo() != result-&gt;cpu_regnrLo() &amp;&amp; opr2-&gt;cpu_regnrLo() != result-&gt;cpu_regnrHi(), "opr2 already overwritten by previous move");
2009       assert(opr2-&gt;cpu_regnrHi() != result-&gt;cpu_regnrLo() &amp;&amp; opr2-&gt;cpu_regnrHi() != result-&gt;cpu_regnrHi(), "opr2 already overwritten by previous move");
2010       __ cmovptr(ncond, result-&gt;as_register_lo(), opr2-&gt;as_register_lo());
2011       NOT_LP64(__ cmovptr(ncond, result-&gt;as_register_hi(), opr2-&gt;as_register_hi());)
2012     } else if (opr2-&gt;is_single_stack()) {
2013       __ cmovl(ncond, result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2014     } else if (opr2-&gt;is_double_stack()) {
2015       __ cmovptr(ncond, result-&gt;as_register_lo(), frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix(), lo_word_offset_in_bytes));
2016       NOT_LP64(__ cmovptr(ncond, result-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix(), hi_word_offset_in_bytes));)
2017     } else {
2018       ShouldNotReachHere();
2019     }
2020 
2021   } else {
2022     Label skip;
2023     __ jcc (acond, skip);
2024     if (opr2-&gt;is_cpu_register()) {
2025       reg2reg(opr2, result);
2026     } else if (opr2-&gt;is_stack()) {
2027       stack2reg(opr2, result, result-&gt;type());
2028     } else if (opr2-&gt;is_constant()) {
2029       const2reg(opr2, result, lir_patch_none, NULL);
2030     } else {
2031       ShouldNotReachHere();
2032     }
2033     __ bind(skip);
2034   }
2035 }
2036 
2037 
2038 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
2039   assert(info == NULL, "should never be used, idiv/irem and ldiv/lrem not handled by this method");
2040 
2041   if (left-&gt;is_single_cpu()) {
2042     assert(left == dest, "left and dest must be equal");
2043     Register lreg = left-&gt;as_register();
2044 
2045     if (right-&gt;is_single_cpu()) {
2046       // cpu register - cpu register
2047       Register rreg = right-&gt;as_register();
2048       switch (code) {
2049         case lir_add: __ addl (lreg, rreg); break;
2050         case lir_sub: __ subl (lreg, rreg); break;
2051         case lir_mul: __ imull(lreg, rreg); break;
2052         default:      ShouldNotReachHere();
2053       }
2054 
2055     } else if (right-&gt;is_stack()) {
2056       // cpu register - stack
2057       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2058       switch (code) {
2059         case lir_add: __ addl(lreg, raddr); break;
2060         case lir_sub: __ subl(lreg, raddr); break;
2061         default:      ShouldNotReachHere();
2062       }
2063 
2064     } else if (right-&gt;is_constant()) {
2065       // cpu register - constant
2066       jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
2067       switch (code) {
2068         case lir_add: {
2069           __ incrementl(lreg, c);
2070           break;
2071         }
2072         case lir_sub: {
2073           __ decrementl(lreg, c);
2074           break;
2075         }
2076         default: ShouldNotReachHere();
2077       }
2078 
2079     } else {
2080       ShouldNotReachHere();
2081     }
2082 
2083   } else if (left-&gt;is_double_cpu()) {
2084     assert(left == dest, "left and dest must be equal");
2085     Register lreg_lo = left-&gt;as_register_lo();
2086     Register lreg_hi = left-&gt;as_register_hi();
2087 
2088     if (right-&gt;is_double_cpu()) {
2089       // cpu register - cpu register
2090       Register rreg_lo = right-&gt;as_register_lo();
2091       Register rreg_hi = right-&gt;as_register_hi();
2092       NOT_LP64(assert_different_registers(lreg_lo, lreg_hi, rreg_lo, rreg_hi));
2093       LP64_ONLY(assert_different_registers(lreg_lo, rreg_lo));
2094       switch (code) {
2095         case lir_add:
2096           __ addptr(lreg_lo, rreg_lo);
2097           NOT_LP64(__ adcl(lreg_hi, rreg_hi));
2098           break;
2099         case lir_sub:
2100           __ subptr(lreg_lo, rreg_lo);
2101           NOT_LP64(__ sbbl(lreg_hi, rreg_hi));
2102           break;
2103         case lir_mul:
2104 #ifdef _LP64
2105           __ imulq(lreg_lo, rreg_lo);
2106 #else
2107           assert(lreg_lo == rax &amp;&amp; lreg_hi == rdx, "must be");
2108           __ imull(lreg_hi, rreg_lo);
2109           __ imull(rreg_hi, lreg_lo);
2110           __ addl (rreg_hi, lreg_hi);
2111           __ mull (rreg_lo);
2112           __ addl (lreg_hi, rreg_hi);
2113 #endif // _LP64
2114           break;
2115         default:
2116           ShouldNotReachHere();
2117       }
2118 
2119     } else if (right-&gt;is_constant()) {
2120       // cpu register - constant
2121 #ifdef _LP64
2122       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong_bits();
2123       __ movptr(r10, (intptr_t) c);
2124       switch (code) {
2125         case lir_add:
2126           __ addptr(lreg_lo, r10);
2127           break;
2128         case lir_sub:
2129           __ subptr(lreg_lo, r10);
2130           break;
2131         default:
2132           ShouldNotReachHere();
2133       }
2134 #else
2135       jint c_lo = right-&gt;as_constant_ptr()-&gt;as_jint_lo();
2136       jint c_hi = right-&gt;as_constant_ptr()-&gt;as_jint_hi();
2137       switch (code) {
2138         case lir_add:
2139           __ addptr(lreg_lo, c_lo);
2140           __ adcl(lreg_hi, c_hi);
2141           break;
2142         case lir_sub:
2143           __ subptr(lreg_lo, c_lo);
2144           __ sbbl(lreg_hi, c_hi);
2145           break;
2146         default:
2147           ShouldNotReachHere();
2148       }
2149 #endif // _LP64
2150 
2151     } else {
2152       ShouldNotReachHere();
2153     }
2154 
2155   } else if (left-&gt;is_single_xmm()) {
2156     assert(left == dest, "left and dest must be equal");
2157     XMMRegister lreg = left-&gt;as_xmm_float_reg();
2158 
2159     if (right-&gt;is_single_xmm()) {
2160       XMMRegister rreg = right-&gt;as_xmm_float_reg();
2161       switch (code) {
2162         case lir_add: __ addss(lreg, rreg);  break;
2163         case lir_sub: __ subss(lreg, rreg);  break;
2164         case lir_mul_strictfp: // fall through
2165         case lir_mul: __ mulss(lreg, rreg);  break;
2166         case lir_div_strictfp: // fall through
2167         case lir_div: __ divss(lreg, rreg);  break;
2168         default: ShouldNotReachHere();
2169       }
2170     } else {
2171       Address raddr;
2172       if (right-&gt;is_single_stack()) {
2173         raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2174       } else if (right-&gt;is_constant()) {
2175         // hack for now
2176         raddr = __ as_Address(InternalAddress(float_constant(right-&gt;as_jfloat())));
2177       } else {
2178         ShouldNotReachHere();
2179       }
2180       switch (code) {
2181         case lir_add: __ addss(lreg, raddr);  break;
2182         case lir_sub: __ subss(lreg, raddr);  break;
2183         case lir_mul_strictfp: // fall through
2184         case lir_mul: __ mulss(lreg, raddr);  break;
2185         case lir_div_strictfp: // fall through
2186         case lir_div: __ divss(lreg, raddr);  break;
2187         default: ShouldNotReachHere();
2188       }
2189     }
2190 
2191   } else if (left-&gt;is_double_xmm()) {
2192     assert(left == dest, "left and dest must be equal");
2193 
2194     XMMRegister lreg = left-&gt;as_xmm_double_reg();
2195     if (right-&gt;is_double_xmm()) {
2196       XMMRegister rreg = right-&gt;as_xmm_double_reg();
2197       switch (code) {
2198         case lir_add: __ addsd(lreg, rreg);  break;
2199         case lir_sub: __ subsd(lreg, rreg);  break;
2200         case lir_mul_strictfp: // fall through
2201         case lir_mul: __ mulsd(lreg, rreg);  break;
2202         case lir_div_strictfp: // fall through
2203         case lir_div: __ divsd(lreg, rreg);  break;
2204         default: ShouldNotReachHere();
2205       }
2206     } else {
2207       Address raddr;
2208       if (right-&gt;is_double_stack()) {
2209         raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
2210       } else if (right-&gt;is_constant()) {
2211         // hack for now
2212         raddr = __ as_Address(InternalAddress(double_constant(right-&gt;as_jdouble())));
2213       } else {
2214         ShouldNotReachHere();
2215       }
2216       switch (code) {
2217         case lir_add: __ addsd(lreg, raddr);  break;
2218         case lir_sub: __ subsd(lreg, raddr);  break;
2219         case lir_mul_strictfp: // fall through
2220         case lir_mul: __ mulsd(lreg, raddr);  break;
2221         case lir_div_strictfp: // fall through
2222         case lir_div: __ divsd(lreg, raddr);  break;
2223         default: ShouldNotReachHere();
2224       }
2225     }
2226 
2227   } else if (left-&gt;is_single_fpu()) {
2228     assert(dest-&gt;is_single_fpu(),  "fpu stack allocation required");
2229 
2230     if (right-&gt;is_single_fpu()) {
2231       arith_fpu_implementation(code, left-&gt;fpu_regnr(), right-&gt;fpu_regnr(), dest-&gt;fpu_regnr(), pop_fpu_stack);
2232 
2233     } else {
2234       assert(left-&gt;fpu_regnr() == 0, "left must be on TOS");
2235       assert(dest-&gt;fpu_regnr() == 0, "dest must be on TOS");
2236 
2237       Address raddr;
2238       if (right-&gt;is_single_stack()) {
2239         raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2240       } else if (right-&gt;is_constant()) {
2241         address const_addr = float_constant(right-&gt;as_jfloat());
2242         assert(const_addr != NULL, "incorrect float/double constant maintainance");
2243         // hack for now
2244         raddr = __ as_Address(InternalAddress(const_addr));
2245       } else {
2246         ShouldNotReachHere();
2247       }
2248 
2249       switch (code) {
2250         case lir_add: __ fadd_s(raddr); break;
2251         case lir_sub: __ fsub_s(raddr); break;
2252         case lir_mul_strictfp: // fall through
2253         case lir_mul: __ fmul_s(raddr); break;
2254         case lir_div_strictfp: // fall through
2255         case lir_div: __ fdiv_s(raddr); break;
2256         default:      ShouldNotReachHere();
2257       }
2258     }
2259 
2260   } else if (left-&gt;is_double_fpu()) {
2261     assert(dest-&gt;is_double_fpu(),  "fpu stack allocation required");
2262 
2263     if (code == lir_mul_strictfp || code == lir_div_strictfp) {
2264       // Double values require special handling for strictfp mul/div on x86
2265       __ fld_x(ExternalAddress(StubRoutines::addr_fpu_subnormal_bias1()));
2266       __ fmulp(left-&gt;fpu_regnrLo() + 1);
2267     }
2268 
2269     if (right-&gt;is_double_fpu()) {
2270       arith_fpu_implementation(code, left-&gt;fpu_regnrLo(), right-&gt;fpu_regnrLo(), dest-&gt;fpu_regnrLo(), pop_fpu_stack);
2271 
2272     } else {
2273       assert(left-&gt;fpu_regnrLo() == 0, "left must be on TOS");
2274       assert(dest-&gt;fpu_regnrLo() == 0, "dest must be on TOS");
2275 
2276       Address raddr;
2277       if (right-&gt;is_double_stack()) {
2278         raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
2279       } else if (right-&gt;is_constant()) {
2280         // hack for now
2281         raddr = __ as_Address(InternalAddress(double_constant(right-&gt;as_jdouble())));
2282       } else {
2283         ShouldNotReachHere();
2284       }
2285 
2286       switch (code) {
2287         case lir_add: __ fadd_d(raddr); break;
2288         case lir_sub: __ fsub_d(raddr); break;
2289         case lir_mul_strictfp: // fall through
2290         case lir_mul: __ fmul_d(raddr); break;
2291         case lir_div_strictfp: // fall through
2292         case lir_div: __ fdiv_d(raddr); break;
2293         default: ShouldNotReachHere();
2294       }
2295     }
2296 
2297     if (code == lir_mul_strictfp || code == lir_div_strictfp) {
2298       // Double values require special handling for strictfp mul/div on x86
2299       __ fld_x(ExternalAddress(StubRoutines::addr_fpu_subnormal_bias2()));
2300       __ fmulp(dest-&gt;fpu_regnrLo() + 1);
2301     }
2302 
2303   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
2304     assert(left == dest, "left and dest must be equal");
2305 
2306     Address laddr;
2307     if (left-&gt;is_single_stack()) {
2308       laddr = frame_map()-&gt;address_for_slot(left-&gt;single_stack_ix());
2309     } else if (left-&gt;is_address()) {
2310       laddr = as_Address(left-&gt;as_address_ptr());
2311     } else {
2312       ShouldNotReachHere();
2313     }
2314 
2315     if (right-&gt;is_single_cpu()) {
2316       Register rreg = right-&gt;as_register();
2317       switch (code) {
2318         case lir_add: __ addl(laddr, rreg); break;
2319         case lir_sub: __ subl(laddr, rreg); break;
2320         default:      ShouldNotReachHere();
2321       }
2322     } else if (right-&gt;is_constant()) {
2323       jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
2324       switch (code) {
2325         case lir_add: {
2326           __ incrementl(laddr, c);
2327           break;
2328         }
2329         case lir_sub: {
2330           __ decrementl(laddr, c);
2331           break;
2332         }
2333         default: ShouldNotReachHere();
2334       }
2335     } else {
2336       ShouldNotReachHere();
2337     }
2338 
2339   } else {
2340     ShouldNotReachHere();
2341   }
2342 }
2343 
2344 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) {
2345   assert(pop_fpu_stack  || (left_index     == dest_index || right_index     == dest_index), "invalid LIR");
2346   assert(!pop_fpu_stack || (left_index - 1 == dest_index || right_index - 1 == dest_index), "invalid LIR");
2347   assert(left_index == 0 || right_index == 0, "either must be on top of stack");
2348 
2349   bool left_is_tos = (left_index == 0);
2350   bool dest_is_tos = (dest_index == 0);
2351   int non_tos_index = (left_is_tos ? right_index : left_index);
2352 
2353   switch (code) {
2354     case lir_add:
2355       if (pop_fpu_stack)       __ faddp(non_tos_index);
2356       else if (dest_is_tos)    __ fadd (non_tos_index);
2357       else                     __ fadda(non_tos_index);
2358       break;
2359 
2360     case lir_sub:
2361       if (left_is_tos) {
2362         if (pop_fpu_stack)     __ fsubrp(non_tos_index);
2363         else if (dest_is_tos)  __ fsub  (non_tos_index);
2364         else                   __ fsubra(non_tos_index);
2365       } else {
2366         if (pop_fpu_stack)     __ fsubp (non_tos_index);
2367         else if (dest_is_tos)  __ fsubr (non_tos_index);
2368         else                   __ fsuba (non_tos_index);
2369       }
2370       break;
2371 
2372     case lir_mul_strictfp: // fall through
2373     case lir_mul:
2374       if (pop_fpu_stack)       __ fmulp(non_tos_index);
2375       else if (dest_is_tos)    __ fmul (non_tos_index);
2376       else                     __ fmula(non_tos_index);
2377       break;
2378 
2379     case lir_div_strictfp: // fall through
2380     case lir_div:
2381       if (left_is_tos) {
2382         if (pop_fpu_stack)     __ fdivrp(non_tos_index);
2383         else if (dest_is_tos)  __ fdiv  (non_tos_index);
2384         else                   __ fdivra(non_tos_index);
2385       } else {
2386         if (pop_fpu_stack)     __ fdivp (non_tos_index);
2387         else if (dest_is_tos)  __ fdivr (non_tos_index);
2388         else                   __ fdiva (non_tos_index);
2389       }
2390       break;
2391 
2392     case lir_rem:
2393       assert(left_is_tos &amp;&amp; dest_is_tos &amp;&amp; right_index == 1, "must be guaranteed by FPU stack allocation");
2394       __ fremr(noreg);
2395       break;
2396 
2397     default:
2398       ShouldNotReachHere();
2399   }
2400 }
2401 
2402 
2403 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr tmp, LIR_Opr dest, LIR_Op* op) {
2404   if (value-&gt;is_double_xmm()) {
2405     switch(code) {
2406       case lir_abs :
2407         {
2408 #ifdef _LP64
2409           if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
2410             assert(tmp-&gt;is_valid(), "need temporary");
2411             __ vpandn(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg(), 2);
2412           } else
2413 #endif
2414           {
2415             if (dest-&gt;as_xmm_double_reg() != value-&gt;as_xmm_double_reg()) {
2416               __ movdbl(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg());
2417             }
2418             assert(!tmp-&gt;is_valid(), "do not need temporary");
2419             __ andpd(dest-&gt;as_xmm_double_reg(),
2420                      ExternalAddress((address)double_signmask_pool));
2421           }
2422         }
2423         break;
2424 
2425       case lir_sqrt: __ sqrtsd(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg()); break;
2426       // all other intrinsics are not available in the SSE instruction set, so FPU is used
2427       default      : ShouldNotReachHere();
2428     }
2429 
2430   } else if (value-&gt;is_double_fpu()) {
2431     assert(value-&gt;fpu_regnrLo() == 0 &amp;&amp; dest-&gt;fpu_regnrLo() == 0, "both must be on TOS");
2432     switch(code) {
2433       case lir_abs   : __ fabs() ; break;
2434       case lir_sqrt  : __ fsqrt(); break;
2435       default      : ShouldNotReachHere();
2436     }
2437   } else {
2438     Unimplemented();
2439   }
2440 }
2441 
2442 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
2443   // assert(left-&gt;destroys_register(), "check");
2444   if (left-&gt;is_single_cpu()) {
2445     Register reg = left-&gt;as_register();
2446     if (right-&gt;is_constant()) {
2447       int val = right-&gt;as_constant_ptr()-&gt;as_jint();
2448       switch (code) {
2449         case lir_logic_and: __ andl (reg, val); break;
2450         case lir_logic_or:  __ orl  (reg, val); break;
2451         case lir_logic_xor: __ xorl (reg, val); break;
2452         default: ShouldNotReachHere();
2453       }
2454     } else if (right-&gt;is_stack()) {
2455       // added support for stack operands
2456       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2457       switch (code) {
2458         case lir_logic_and: __ andl (reg, raddr); break;
2459         case lir_logic_or:  __ orl  (reg, raddr); break;
2460         case lir_logic_xor: __ xorl (reg, raddr); break;
2461         default: ShouldNotReachHere();
2462       }
2463     } else {
2464       Register rright = right-&gt;as_register();
2465       switch (code) {
2466         case lir_logic_and: __ andptr (reg, rright); break;
2467         case lir_logic_or : __ orptr  (reg, rright); break;
2468         case lir_logic_xor: __ xorptr (reg, rright); break;
2469         default: ShouldNotReachHere();
2470       }
2471     }
2472     move_regs(reg, dst-&gt;as_register());
2473   } else {
2474     Register l_lo = left-&gt;as_register_lo();
2475     Register l_hi = left-&gt;as_register_hi();
2476     if (right-&gt;is_constant()) {
2477 #ifdef _LP64
2478       __ mov64(rscratch1, right-&gt;as_constant_ptr()-&gt;as_jlong());
2479       switch (code) {
2480         case lir_logic_and:
2481           __ andq(l_lo, rscratch1);
2482           break;
2483         case lir_logic_or:
2484           __ orq(l_lo, rscratch1);
2485           break;
2486         case lir_logic_xor:
2487           __ xorq(l_lo, rscratch1);
2488           break;
2489         default: ShouldNotReachHere();
2490       }
2491 #else
2492       int r_lo = right-&gt;as_constant_ptr()-&gt;as_jint_lo();
2493       int r_hi = right-&gt;as_constant_ptr()-&gt;as_jint_hi();
2494       switch (code) {
2495         case lir_logic_and:
2496           __ andl(l_lo, r_lo);
2497           __ andl(l_hi, r_hi);
2498           break;
2499         case lir_logic_or:
2500           __ orl(l_lo, r_lo);
2501           __ orl(l_hi, r_hi);
2502           break;
2503         case lir_logic_xor:
2504           __ xorl(l_lo, r_lo);
2505           __ xorl(l_hi, r_hi);
2506           break;
2507         default: ShouldNotReachHere();
2508       }
2509 #endif // _LP64
2510     } else {
2511 #ifdef _LP64
2512       Register r_lo;
2513       if (is_reference_type(right-&gt;type())) {
2514         r_lo = right-&gt;as_register();
2515       } else {
2516         r_lo = right-&gt;as_register_lo();
2517       }
2518 #else
2519       Register r_lo = right-&gt;as_register_lo();
2520       Register r_hi = right-&gt;as_register_hi();
2521       assert(l_lo != r_hi, "overwriting registers");
2522 #endif
2523       switch (code) {
2524         case lir_logic_and:
2525           __ andptr(l_lo, r_lo);
2526           NOT_LP64(__ andptr(l_hi, r_hi);)
2527           break;
2528         case lir_logic_or:
2529           __ orptr(l_lo, r_lo);
2530           NOT_LP64(__ orptr(l_hi, r_hi);)
2531           break;
2532         case lir_logic_xor:
2533           __ xorptr(l_lo, r_lo);
2534           NOT_LP64(__ xorptr(l_hi, r_hi);)
2535           break;
2536         default: ShouldNotReachHere();
2537       }
2538     }
2539 
2540     Register dst_lo = dst-&gt;as_register_lo();
2541     Register dst_hi = dst-&gt;as_register_hi();
2542 
2543 #ifdef _LP64
2544     move_regs(l_lo, dst_lo);
2545 #else
2546     if (dst_lo == l_hi) {
2547       assert(dst_hi != l_lo, "overwriting registers");
2548       move_regs(l_hi, dst_hi);
2549       move_regs(l_lo, dst_lo);
2550     } else {
2551       assert(dst_lo != l_hi, "overwriting registers");
2552       move_regs(l_lo, dst_lo);
2553       move_regs(l_hi, dst_hi);
2554     }
2555 #endif // _LP64
2556   }
2557 }
2558 
2559 
2560 // we assume that rax, and rdx can be overwritten
2561 void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr temp, LIR_Opr result, CodeEmitInfo* info) {
2562 
2563   assert(left-&gt;is_single_cpu(),   "left must be register");
2564   assert(right-&gt;is_single_cpu() || right-&gt;is_constant(),  "right must be register or constant");
2565   assert(result-&gt;is_single_cpu(), "result must be register");
2566 
2567   //  assert(left-&gt;destroys_register(), "check");
2568   //  assert(right-&gt;destroys_register(), "check");
2569 
2570   Register lreg = left-&gt;as_register();
2571   Register dreg = result-&gt;as_register();
2572 
2573   if (right-&gt;is_constant()) {
2574     jint divisor = right-&gt;as_constant_ptr()-&gt;as_jint();
2575     assert(divisor &gt; 0 &amp;&amp; is_power_of_2(divisor), "must be");
2576     if (code == lir_idiv) {
2577       assert(lreg == rax, "must be rax,");
2578       assert(temp-&gt;as_register() == rdx, "tmp register must be rdx");
2579       __ cdql(); // sign extend into rdx:rax
2580       if (divisor == 2) {
2581         __ subl(lreg, rdx);
2582       } else {
2583         __ andl(rdx, divisor - 1);
2584         __ addl(lreg, rdx);
2585       }
2586       __ sarl(lreg, log2_jint(divisor));
2587       move_regs(lreg, dreg);
2588     } else if (code == lir_irem) {
2589       Label done;
2590       __ mov(dreg, lreg);
2591       __ andl(dreg, 0x80000000 | (divisor - 1));
2592       __ jcc(Assembler::positive, done);
2593       __ decrement(dreg);
2594       __ orl(dreg, ~(divisor - 1));
2595       __ increment(dreg);
2596       __ bind(done);
2597     } else {
2598       ShouldNotReachHere();
2599     }
2600   } else {
2601     Register rreg = right-&gt;as_register();
2602     assert(lreg == rax, "left register must be rax,");
2603     assert(rreg != rdx, "right register must not be rdx");
2604     assert(temp-&gt;as_register() == rdx, "tmp register must be rdx");
2605 
2606     move_regs(lreg, rax);
2607 
2608     int idivl_offset = __ corrected_idivl(rreg);
2609     if (ImplicitDiv0Checks) {
2610       add_debug_info_for_div0(idivl_offset, info);
2611     }
2612     if (code == lir_irem) {
2613       move_regs(rdx, dreg); // result is in rdx
2614     } else {
2615       move_regs(rax, dreg);
2616     }
2617   }
2618 }
2619 
2620 
2621 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
2622   if (opr1-&gt;is_single_cpu()) {
2623     Register reg1 = opr1-&gt;as_register();
2624     if (opr2-&gt;is_single_cpu()) {
2625       // cpu register - cpu register
2626       if (is_reference_type(opr1-&gt;type())) {
2627         __ cmpoop(reg1, opr2-&gt;as_register());
2628       } else {
2629         assert(!is_reference_type(opr2-&gt;type()), "cmp int, oop?");
2630         __ cmpl(reg1, opr2-&gt;as_register());
2631       }
2632     } else if (opr2-&gt;is_stack()) {
2633       // cpu register - stack
2634       if (is_reference_type(opr1-&gt;type())) {
2635         __ cmpoop(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2636       } else {
2637         __ cmpl(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2638       }
2639     } else if (opr2-&gt;is_constant()) {
2640       // cpu register - constant
2641       LIR_Const* c = opr2-&gt;as_constant_ptr();
2642       if (c-&gt;type() == T_INT) {
2643         __ cmpl(reg1, c-&gt;as_jint());
2644       } else if (c-&gt;type() == T_METADATA) {
2645         // All we need for now is a comparison with NULL for equality.
2646         assert(condition == lir_cond_equal || condition == lir_cond_notEqual, "oops");
2647         Metadata* m = c-&gt;as_metadata();
2648         if (m == NULL) {
2649           __ cmpptr(reg1, (int32_t)0);
2650         } else {
2651           ShouldNotReachHere();
2652         }
2653       } else if (is_reference_type(c-&gt;type())) {
2654         // In 64bit oops are single register
2655         jobject o = c-&gt;as_jobject();
2656         if (o == NULL) {
2657           __ cmpptr(reg1, (int32_t)NULL_WORD);
2658         } else {
2659           __ cmpoop(reg1, o);
2660         }
2661       } else {
2662         fatal("unexpected type: %s", basictype_to_str(c-&gt;type()));
2663       }
2664       // cpu register - address
2665     } else if (opr2-&gt;is_address()) {
2666       if (op-&gt;info() != NULL) {
2667         add_debug_info_for_null_check_here(op-&gt;info());
2668       }
2669       __ cmpl(reg1, as_Address(opr2-&gt;as_address_ptr()));
2670     } else {
2671       ShouldNotReachHere();
2672     }
2673 
2674   } else if(opr1-&gt;is_double_cpu()) {
2675     Register xlo = opr1-&gt;as_register_lo();
2676     Register xhi = opr1-&gt;as_register_hi();
2677     if (opr2-&gt;is_double_cpu()) {
2678 #ifdef _LP64
2679       __ cmpptr(xlo, opr2-&gt;as_register_lo());
2680 #else
2681       // cpu register - cpu register
2682       Register ylo = opr2-&gt;as_register_lo();
2683       Register yhi = opr2-&gt;as_register_hi();
2684       __ subl(xlo, ylo);
2685       __ sbbl(xhi, yhi);
2686       if (condition == lir_cond_equal || condition == lir_cond_notEqual) {
2687         __ orl(xhi, xlo);
2688       }
2689 #endif // _LP64
2690     } else if (opr2-&gt;is_constant()) {
2691       // cpu register - constant 0
2692       assert(opr2-&gt;as_jlong() == (jlong)0, "only handles zero");
2693 #ifdef _LP64
2694       __ cmpptr(xlo, (int32_t)opr2-&gt;as_jlong());
2695 #else
2696       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, "only handles equals case");
2697       __ orl(xhi, xlo);
2698 #endif // _LP64
2699     } else {
2700       ShouldNotReachHere();
2701     }
2702 
2703   } else if (opr1-&gt;is_single_xmm()) {
2704     XMMRegister reg1 = opr1-&gt;as_xmm_float_reg();
2705     if (opr2-&gt;is_single_xmm()) {
2706       // xmm register - xmm register
2707       __ ucomiss(reg1, opr2-&gt;as_xmm_float_reg());
2708     } else if (opr2-&gt;is_stack()) {
2709       // xmm register - stack
2710       __ ucomiss(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2711     } else if (opr2-&gt;is_constant()) {
2712       // xmm register - constant
2713       __ ucomiss(reg1, InternalAddress(float_constant(opr2-&gt;as_jfloat())));
2714     } else if (opr2-&gt;is_address()) {
2715       // xmm register - address
2716       if (op-&gt;info() != NULL) {
2717         add_debug_info_for_null_check_here(op-&gt;info());
2718       }
2719       __ ucomiss(reg1, as_Address(opr2-&gt;as_address_ptr()));
2720     } else {
2721       ShouldNotReachHere();
2722     }
2723 
2724   } else if (opr1-&gt;is_double_xmm()) {
2725     XMMRegister reg1 = opr1-&gt;as_xmm_double_reg();
2726     if (opr2-&gt;is_double_xmm()) {
2727       // xmm register - xmm register
2728       __ ucomisd(reg1, opr2-&gt;as_xmm_double_reg());
2729     } else if (opr2-&gt;is_stack()) {
2730       // xmm register - stack
2731       __ ucomisd(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix()));
2732     } else if (opr2-&gt;is_constant()) {
2733       // xmm register - constant
2734       __ ucomisd(reg1, InternalAddress(double_constant(opr2-&gt;as_jdouble())));
2735     } else if (opr2-&gt;is_address()) {
2736       // xmm register - address
2737       if (op-&gt;info() != NULL) {
2738         add_debug_info_for_null_check_here(op-&gt;info());
2739       }
2740       __ ucomisd(reg1, as_Address(opr2-&gt;pointer()-&gt;as_address()));
2741     } else {
2742       ShouldNotReachHere();
2743     }
2744 
2745   } else if(opr1-&gt;is_single_fpu() || opr1-&gt;is_double_fpu()) {
2746     assert(opr1-&gt;is_fpu_register() &amp;&amp; opr1-&gt;fpu() == 0, "currently left-hand side must be on TOS (relax this restriction)");
2747     assert(opr2-&gt;is_fpu_register(), "both must be registers");
2748     __ fcmp(noreg, opr2-&gt;fpu(), op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);
2749 
2750   } else if (opr1-&gt;is_address() &amp;&amp; opr2-&gt;is_constant()) {
2751     LIR_Const* c = opr2-&gt;as_constant_ptr();
2752 #ifdef _LP64
2753     if (is_reference_type(c-&gt;type())) {
2754       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, "need to reverse");
2755       __ movoop(rscratch1, c-&gt;as_jobject());
2756     }
2757 #endif // LP64
2758     if (op-&gt;info() != NULL) {
2759       add_debug_info_for_null_check_here(op-&gt;info());
2760     }
2761     // special case: address - constant
2762     LIR_Address* addr = opr1-&gt;as_address_ptr();
2763     if (c-&gt;type() == T_INT) {
2764       __ cmpl(as_Address(addr), c-&gt;as_jint());
2765     } else if (is_reference_type(c-&gt;type())) {
2766 #ifdef _LP64
2767       // %%% Make this explode if addr isn't reachable until we figure out a
2768       // better strategy by giving noreg as the temp for as_Address
2769       __ cmpoop(rscratch1, as_Address(addr, noreg));
2770 #else
2771       __ cmpoop(as_Address(addr), c-&gt;as_jobject());
2772 #endif // _LP64
2773     } else {
2774       ShouldNotReachHere();
2775     }
2776 
2777   } else {
2778     ShouldNotReachHere();
2779   }
2780 }
2781 
2782 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
2783   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
2784     if (left-&gt;is_single_xmm()) {
2785       assert(right-&gt;is_single_xmm(), "must match");
2786       __ cmpss2int(left-&gt;as_xmm_float_reg(), right-&gt;as_xmm_float_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
2787     } else if (left-&gt;is_double_xmm()) {
2788       assert(right-&gt;is_double_xmm(), "must match");
2789       __ cmpsd2int(left-&gt;as_xmm_double_reg(), right-&gt;as_xmm_double_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
2790 
2791     } else {
2792       assert(left-&gt;is_single_fpu() || left-&gt;is_double_fpu(), "must be");
2793       assert(right-&gt;is_single_fpu() || right-&gt;is_double_fpu(), "must match");
2794 
2795       assert(left-&gt;fpu() == 0, "left must be on TOS");
2796       __ fcmp2int(dst-&gt;as_register(), code == lir_ucmp_fd2i, right-&gt;fpu(),
2797                   op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);
2798     }
2799   } else {
2800     assert(code == lir_cmp_l2i, "check");
2801 #ifdef _LP64
2802     Label done;
2803     Register dest = dst-&gt;as_register();
2804     __ cmpptr(left-&gt;as_register_lo(), right-&gt;as_register_lo());
2805     __ movl(dest, -1);
2806     __ jccb(Assembler::less, done);
2807     __ set_byte_if_not_zero(dest);
2808     __ movzbl(dest, dest);
2809     __ bind(done);
2810 #else
2811     __ lcmp2int(left-&gt;as_register_hi(),
2812                 left-&gt;as_register_lo(),
2813                 right-&gt;as_register_hi(),
2814                 right-&gt;as_register_lo());
2815     move_regs(left-&gt;as_register_hi(), dst-&gt;as_register());
2816 #endif // _LP64
2817   }
2818 }
2819 
2820 
2821 void LIR_Assembler::align_call(LIR_Code code) {
2822   // make sure that the displacement word of the call ends up word aligned
2823   int offset = __ offset();
2824   switch (code) {
2825   case lir_static_call:
2826   case lir_optvirtual_call:
2827   case lir_dynamic_call:
2828     offset += NativeCall::displacement_offset;
2829     break;
2830   case lir_icvirtual_call:
2831     offset += NativeCall::displacement_offset + NativeMovConstReg::instruction_size;
2832     break;
2833   case lir_virtual_call:  // currently, sparc-specific for niagara
2834   default: ShouldNotReachHere();
2835   }
2836   __ align(BytesPerWord, offset);
2837 }
2838 
2839 
2840 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
2841   assert((__ offset() + NativeCall::displacement_offset) % BytesPerWord == 0,
2842          "must be aligned");
2843   __ call(AddressLiteral(op-&gt;addr(), rtype));
2844   add_call_info(code_offset(), op-&gt;info());
2845 }
2846 
2847 
2848 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
2849   __ ic_call(op-&gt;addr());
2850   add_call_info(code_offset(), op-&gt;info());
2851   assert((__ offset() - NativeCall::instruction_size + NativeCall::displacement_offset) % BytesPerWord == 0,
2852          "must be aligned");
2853 }
2854 
2855 
2856 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2857 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2858   ShouldNotReachHere();
2859 }
2860 
2861 
2862 void LIR_Assembler::emit_static_call_stub() {
2863   address call_pc = __ pc();
2864   address stub = __ start_a_stub(call_stub_size());
2865   if (stub == NULL) {
2866     bailout("static call stub overflow");
2867     return;
2868   }
2869 
2870   int start = __ offset();
2871 
2872   // make sure that the displacement word of the call ends up word aligned
2873   __ align(BytesPerWord, __ offset() + NativeMovConstReg::instruction_size + NativeCall::displacement_offset);
2874   __ relocate(static_stub_Relocation::spec(call_pc, false /* is_aot */));
2875   __ mov_metadata(rbx, (Metadata*)NULL);
2876   // must be set to -1 at code generation time
2877   assert(((__ offset() + 1) % BytesPerWord) == 0, "must be aligned");
2878   // On 64bit this will die since it will take a movq &amp; jmp, must be only a jmp
2879   __ jump(RuntimeAddress(__ pc()));
2880 
2881   if (UseAOT) {
2882     // Trampoline to aot code
2883     __ relocate(static_stub_Relocation::spec(call_pc, true /* is_aot */));
2884 #ifdef _LP64
2885     __ mov64(rax, CONST64(0));  // address is zapped till fixup time.
2886 #else
2887     __ movl(rax, 0xdeadffff);  // address is zapped till fixup time.
2888 #endif
2889     __ jmp(rax);
2890   }
2891   assert(__ offset() - start &lt;= call_stub_size(), "stub too big");
2892   __ end_a_stub();
2893 }
2894 
2895 
2896 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
2897   assert(exceptionOop-&gt;as_register() == rax, "must match");
2898   assert(exceptionPC-&gt;as_register() == rdx, "must match");
2899 
2900   // exception object is not added to oop map by LinearScan
2901   // (LinearScan assumes that no oops are in fixed registers)
2902   info-&gt;add_register_oop(exceptionOop);
2903   Runtime1::StubID unwind_id;
2904 
2905   // get current pc information
2906   // pc is only needed if the method has an exception handler, the unwind code does not need it.
2907   int pc_for_athrow_offset = __ offset();
2908   InternalAddress pc_for_athrow(__ pc());
2909   __ lea(exceptionPC-&gt;as_register(), pc_for_athrow);
2910   add_call_info(pc_for_athrow_offset, info); // for exception handler
2911 
2912   __ verify_not_null_oop(rax);
2913   // search an exception handler (rax: exception oop, rdx: throwing pc)
2914   if (compilation()-&gt;has_fpu_code()) {
2915     unwind_id = Runtime1::handle_exception_id;
2916   } else {
2917     unwind_id = Runtime1::handle_exception_nofpu_id;
2918   }
2919   __ call(RuntimeAddress(Runtime1::entry_for(unwind_id)));
2920 
2921   // enough room for two byte trap
2922   __ nop();
2923 }
2924 
2925 
2926 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
2927   assert(exceptionOop-&gt;as_register() == rax, "must match");
2928 
2929   __ jmp(_unwind_handler_entry);
2930 }
2931 
2932 
2933 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2934 
2935   // optimized version for linear scan:
2936   // * count must be already in ECX (guaranteed by LinearScan)
2937   // * left and dest must be equal
2938   // * tmp must be unused
2939   assert(count-&gt;as_register() == SHIFT_count, "count must be in ECX");
2940   assert(left == dest, "left and dest must be equal");
2941   assert(tmp-&gt;is_illegal(), "wasting a register if tmp is allocated");
2942 
2943   if (left-&gt;is_single_cpu()) {
2944     Register value = left-&gt;as_register();
2945     assert(value != SHIFT_count, "left cannot be ECX");
2946 
2947     switch (code) {
2948       case lir_shl:  __ shll(value); break;
2949       case lir_shr:  __ sarl(value); break;
2950       case lir_ushr: __ shrl(value); break;
2951       default: ShouldNotReachHere();
2952     }
2953   } else if (left-&gt;is_double_cpu()) {
2954     Register lo = left-&gt;as_register_lo();
2955     Register hi = left-&gt;as_register_hi();
2956     assert(lo != SHIFT_count &amp;&amp; hi != SHIFT_count, "left cannot be ECX");
2957 #ifdef _LP64
2958     switch (code) {
2959       case lir_shl:  __ shlptr(lo);        break;
2960       case lir_shr:  __ sarptr(lo);        break;
2961       case lir_ushr: __ shrptr(lo);        break;
2962       default: ShouldNotReachHere();
2963     }
2964 #else
2965 
2966     switch (code) {
2967       case lir_shl:  __ lshl(hi, lo);        break;
2968       case lir_shr:  __ lshr(hi, lo, true);  break;
2969       case lir_ushr: __ lshr(hi, lo, false); break;
2970       default: ShouldNotReachHere();
2971     }
2972 #endif // LP64
2973   } else {
2974     ShouldNotReachHere();
2975   }
2976 }
2977 
2978 
2979 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2980   if (dest-&gt;is_single_cpu()) {
2981     // first move left into dest so that left is not destroyed by the shift
2982     Register value = dest-&gt;as_register();
2983     count = count &amp; 0x1F; // Java spec
2984 
2985     move_regs(left-&gt;as_register(), value);
2986     switch (code) {
2987       case lir_shl:  __ shll(value, count); break;
2988       case lir_shr:  __ sarl(value, count); break;
2989       case lir_ushr: __ shrl(value, count); break;
2990       default: ShouldNotReachHere();
2991     }
2992   } else if (dest-&gt;is_double_cpu()) {
2993 #ifndef _LP64
2994     Unimplemented();
2995 #else
2996     // first move left into dest so that left is not destroyed by the shift
2997     Register value = dest-&gt;as_register_lo();
2998     count = count &amp; 0x1F; // Java spec
2999 
3000     move_regs(left-&gt;as_register_lo(), value);
3001     switch (code) {
3002       case lir_shl:  __ shlptr(value, count); break;
3003       case lir_shr:  __ sarptr(value, count); break;
3004       case lir_ushr: __ shrptr(value, count); break;
3005       default: ShouldNotReachHere();
3006     }
3007 #endif // _LP64
3008   } else {
3009     ShouldNotReachHere();
3010   }
3011 }
3012 
3013 
3014 void LIR_Assembler::store_parameter(Register r, int offset_from_rsp_in_words) {
3015   assert(offset_from_rsp_in_words &gt;= 0, "invalid offset from rsp");
3016   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3017   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), "invalid offset");
3018   __ movptr (Address(rsp, offset_from_rsp_in_bytes), r);
3019 }
3020 
3021 
3022 void LIR_Assembler::store_parameter(jint c,     int offset_from_rsp_in_words) {
3023   assert(offset_from_rsp_in_words &gt;= 0, "invalid offset from rsp");
3024   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3025   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), "invalid offset");
3026   __ movptr (Address(rsp, offset_from_rsp_in_bytes), c);
3027 }
3028 
3029 
3030 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
3031   assert(offset_from_rsp_in_words &gt;= 0, "invalid offset from rsp");
3032   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3033   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), "invalid offset");
3034   __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);
3035 }
3036 
3037 
3038 void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {
3039   assert(offset_from_rsp_in_words &gt;= 0, "invalid offset from rsp");
3040   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3041   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), "invalid offset");
3042   __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);
3043 }
3044 
3045 
3046 // This code replaces a call to arraycopy; no exception may
3047 // be thrown in this code, they must be thrown in the System.arraycopy
3048 // activation frame; we could save some checks if this would not be the case
3049 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3050   ciArrayKlass* default_type = op-&gt;expected_type();
3051   Register src = op-&gt;src()-&gt;as_register();
3052   Register dst = op-&gt;dst()-&gt;as_register();
3053   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3054   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3055   Register length  = op-&gt;length()-&gt;as_register();
3056   Register tmp = op-&gt;tmp()-&gt;as_register();
3057 
3058   __ resolve(ACCESS_READ, src);
3059   __ resolve(ACCESS_WRITE, dst);
3060 
3061   CodeStub* stub = op-&gt;stub();
3062   int flags = op-&gt;flags();
3063   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
3064   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
3065 
3066   // if we don't know anything, just go through the generic arraycopy
3067   if (default_type == NULL) {
3068     // save outgoing arguments on stack in case call to System.arraycopy is needed
3069     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3070     // for interpreter calling conventions. Now we have to do it in new style conventions.
3071     // For the moment until C1 gets the new register allocator I just force all the
3072     // args to the right place (except the register args) and then on the back side
3073     // reload the register args properly if we go slow path. Yuck
3074 
3075     // These are proper for the calling convention
3076     store_parameter(length, 2);
3077     store_parameter(dst_pos, 1);
3078     store_parameter(dst, 0);
3079 
3080     // these are just temporary placements until we need to reload
3081     store_parameter(src_pos, 3);
3082     store_parameter(src, 4);
3083     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, "mismatch in calling convention");)
3084 
3085     address copyfunc_addr = StubRoutines::generic_arraycopy();
3086     assert(copyfunc_addr != NULL, "generic arraycopy stub required");
3087 
3088     // pass arguments: may push as this is not a safepoint; SP must be fix at each safepoint
3089 #ifdef _LP64
3090     // The arguments are in java calling convention so we can trivially shift them to C
3091     // convention
3092     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
3093     __ mov(c_rarg0, j_rarg0);
3094     assert_different_registers(c_rarg1, j_rarg2, j_rarg3, j_rarg4);
3095     __ mov(c_rarg1, j_rarg1);
3096     assert_different_registers(c_rarg2, j_rarg3, j_rarg4);
3097     __ mov(c_rarg2, j_rarg2);
3098     assert_different_registers(c_rarg3, j_rarg4);
3099     __ mov(c_rarg3, j_rarg3);
3100 #ifdef _WIN64
3101     // Allocate abi space for args but be sure to keep stack aligned
3102     __ subptr(rsp, 6*wordSize);
3103     store_parameter(j_rarg4, 4);
3104 #ifndef PRODUCT
3105     if (PrintC1Statistics) {
3106       __ incrementl(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
3107     }
3108 #endif
3109     __ call(RuntimeAddress(copyfunc_addr));
3110     __ addptr(rsp, 6*wordSize);
3111 #else
3112     __ mov(c_rarg4, j_rarg4);
3113 #ifndef PRODUCT
3114     if (PrintC1Statistics) {
3115       __ incrementl(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
3116     }
3117 #endif
3118     __ call(RuntimeAddress(copyfunc_addr));
3119 #endif // _WIN64
3120 #else
3121     __ push(length);
3122     __ push(dst_pos);
3123     __ push(dst);
3124     __ push(src_pos);
3125     __ push(src);
3126 
3127 #ifndef PRODUCT
3128     if (PrintC1Statistics) {
3129       __ incrementl(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
3130     }
3131 #endif
3132     __ call_VM_leaf(copyfunc_addr, 5); // removes pushed parameter from the stack
3133 
3134 #endif // _LP64
3135 
3136     __ cmpl(rax, 0);
3137     __ jcc(Assembler::equal, *stub-&gt;continuation());
3138 
3139     __ mov(tmp, rax);
3140     __ xorl(tmp, -1);
3141 
3142     // Reload values from the stack so they are where the stub
3143     // expects them.
3144     __ movptr   (dst,     Address(rsp, 0*BytesPerWord));
3145     __ movptr   (dst_pos, Address(rsp, 1*BytesPerWord));
3146     __ movptr   (length,  Address(rsp, 2*BytesPerWord));
3147     __ movptr   (src_pos, Address(rsp, 3*BytesPerWord));
3148     __ movptr   (src,     Address(rsp, 4*BytesPerWord));
3149 
3150     __ subl(length, tmp);
3151     __ addl(src_pos, tmp);
3152     __ addl(dst_pos, tmp);
3153     __ jmp(*stub-&gt;entry());
3154 
3155     __ bind(*stub-&gt;continuation());
3156     return;
3157   }
3158 
3159   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), "must be true at this point");
3160 
3161   int elem_size = type2aelembytes(basic_type);
3162   Address::ScaleFactor scale;
3163 
3164   switch (elem_size) {
3165     case 1 :
3166       scale = Address::times_1;
3167       break;
3168     case 2 :
3169       scale = Address::times_2;
3170       break;
3171     case 4 :
3172       scale = Address::times_4;
3173       break;
3174     case 8 :
3175       scale = Address::times_8;
3176       break;
3177     default:
3178       scale = Address::no_scale;
3179       ShouldNotReachHere();
3180   }
3181 
3182   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
3183   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
3184   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
3185   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
3186 
3187   // length and pos's are all sign extended at this point on 64bit
3188 
3189   // test for NULL
3190   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
3191     __ testptr(src, src);
3192     __ jcc(Assembler::zero, *stub-&gt;entry());
3193   }
3194   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
3195     __ testptr(dst, dst);
3196     __ jcc(Assembler::zero, *stub-&gt;entry());
3197   }
3198 
3199   // If the compiler was not able to prove that exact type of the source or the destination
3200   // of the arraycopy is an array type, check at runtime if the source or the destination is
3201   // an instance type.
3202   if (flags &amp; LIR_OpArrayCopy::type_check) {
3203     if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
3204       __ load_klass(tmp, dst);
3205       __ cmpl(Address(tmp, in_bytes(Klass::layout_helper_offset())), Klass::_lh_neutral_value);
3206       __ jcc(Assembler::greaterEqual, *stub-&gt;entry());
3207     }
3208 
3209     if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
3210       __ load_klass(tmp, src);
3211       __ cmpl(Address(tmp, in_bytes(Klass::layout_helper_offset())), Klass::_lh_neutral_value);
3212       __ jcc(Assembler::greaterEqual, *stub-&gt;entry());
3213     }
3214   }
3215 
3216   // check if negative
3217   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
3218     __ testl(src_pos, src_pos);
3219     __ jcc(Assembler::less, *stub-&gt;entry());
3220   }
3221   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
3222     __ testl(dst_pos, dst_pos);
3223     __ jcc(Assembler::less, *stub-&gt;entry());
3224   }
3225 
3226   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
3227     __ lea(tmp, Address(src_pos, length, Address::times_1, 0));
3228     __ cmpl(tmp, src_length_addr);
3229     __ jcc(Assembler::above, *stub-&gt;entry());
3230   }
3231   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
3232     __ lea(tmp, Address(dst_pos, length, Address::times_1, 0));
3233     __ cmpl(tmp, dst_length_addr);
3234     __ jcc(Assembler::above, *stub-&gt;entry());
3235   }
3236 
3237   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
3238     __ testl(length, length);
3239     __ jcc(Assembler::less, *stub-&gt;entry());
3240   }
3241 
3242 #ifdef _LP64
3243   __ movl2ptr(src_pos, src_pos); //higher 32bits must be null
3244   __ movl2ptr(dst_pos, dst_pos); //higher 32bits must be null
3245 #endif
3246 
3247   if (flags &amp; LIR_OpArrayCopy::type_check) {
3248     // We don't know the array types are compatible
3249     if (basic_type != T_OBJECT) {
3250       // Simple test for basic type arrays
3251       if (UseCompressedClassPointers) {
3252         __ movl(tmp, src_klass_addr);
3253         __ cmpl(tmp, dst_klass_addr);
3254       } else {
3255         __ movptr(tmp, src_klass_addr);
3256         __ cmpptr(tmp, dst_klass_addr);
3257       }
3258       __ jcc(Assembler::notEqual, *stub-&gt;entry());
3259     } else {
3260       // For object arrays, if src is a sub class of dst then we can
3261       // safely do the copy.
3262       Label cont, slow;
3263 
3264       __ push(src);
3265       __ push(dst);
3266 
3267       __ load_klass(src, src);
3268       __ load_klass(dst, dst);
3269 
3270       __ check_klass_subtype_fast_path(src, dst, tmp, &amp;cont, &amp;slow, NULL);
3271 
3272       __ push(src);
3273       __ push(dst);
3274       __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
3275       __ pop(dst);
3276       __ pop(src);
3277 
3278       __ cmpl(src, 0);
3279       __ jcc(Assembler::notEqual, cont);
3280 
3281       __ bind(slow);
3282       __ pop(dst);
3283       __ pop(src);
3284 
3285       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
3286       if (copyfunc_addr != NULL) { // use stub if available
3287         // src is not a sub class of dst so we have to do a
3288         // per-element check.
3289 
3290         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
3291         if ((flags &amp; mask) != mask) {
3292           // Check that at least both of them object arrays.
3293           assert(flags &amp; mask, "one of the two should be known to be an object array");
3294 
3295           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
3296             __ load_klass(tmp, src);
3297           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
3298             __ load_klass(tmp, dst);
3299           }
3300           int lh_offset = in_bytes(Klass::layout_helper_offset());
3301           Address klass_lh_addr(tmp, lh_offset);
3302           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
3303           __ cmpl(klass_lh_addr, objArray_lh);
3304           __ jcc(Assembler::notEqual, *stub-&gt;entry());
3305         }
3306 
3307        // Spill because stubs can use any register they like and it's
3308        // easier to restore just those that we care about.
3309        store_parameter(dst, 0);
3310        store_parameter(dst_pos, 1);
3311        store_parameter(length, 2);
3312        store_parameter(src_pos, 3);
3313        store_parameter(src, 4);
3314 
3315 #ifndef _LP64
3316         __ movptr(tmp, dst_klass_addr);
3317         __ movptr(tmp, Address(tmp, ObjArrayKlass::element_klass_offset()));
3318         __ push(tmp);
3319         __ movl(tmp, Address(tmp, Klass::super_check_offset_offset()));
3320         __ push(tmp);
3321         __ push(length);
3322         __ lea(tmp, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3323         __ push(tmp);
3324         __ lea(tmp, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3325         __ push(tmp);
3326 
3327         __ call_VM_leaf(copyfunc_addr, 5);
3328 #else
3329         __ movl2ptr(length, length); //higher 32bits must be null
3330 
3331         __ lea(c_rarg0, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3332         assert_different_registers(c_rarg0, dst, dst_pos, length);
3333         __ lea(c_rarg1, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3334         assert_different_registers(c_rarg1, dst, length);
3335 
3336         __ mov(c_rarg2, length);
3337         assert_different_registers(c_rarg2, dst);
3338 
3339 #ifdef _WIN64
3340         // Allocate abi space for args but be sure to keep stack aligned
3341         __ subptr(rsp, 6*wordSize);
3342         __ load_klass(c_rarg3, dst);
3343         __ movptr(c_rarg3, Address(c_rarg3, ObjArrayKlass::element_klass_offset()));
3344         store_parameter(c_rarg3, 4);
3345         __ movl(c_rarg3, Address(c_rarg3, Klass::super_check_offset_offset()));
3346         __ call(RuntimeAddress(copyfunc_addr));
3347         __ addptr(rsp, 6*wordSize);
3348 #else
3349         __ load_klass(c_rarg4, dst);
3350         __ movptr(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));
3351         __ movl(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));
3352         __ call(RuntimeAddress(copyfunc_addr));
3353 #endif
3354 
3355 #endif
3356 
3357 #ifndef PRODUCT
3358         if (PrintC1Statistics) {
3359           Label failed;
3360           __ testl(rax, rax);
3361           __ jcc(Assembler::notZero, failed);
3362           __ incrementl(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_cnt));
3363           __ bind(failed);
3364         }
3365 #endif
3366 
3367         __ testl(rax, rax);
3368         __ jcc(Assembler::zero, *stub-&gt;continuation());
3369 
3370 #ifndef PRODUCT
3371         if (PrintC1Statistics) {
3372           __ incrementl(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt));
3373         }
3374 #endif
3375 
3376         __ mov(tmp, rax);
3377 
3378         __ xorl(tmp, -1);
3379 
3380         // Restore previously spilled arguments
3381         __ movptr   (dst,     Address(rsp, 0*BytesPerWord));
3382         __ movptr   (dst_pos, Address(rsp, 1*BytesPerWord));
3383         __ movptr   (length,  Address(rsp, 2*BytesPerWord));
3384         __ movptr   (src_pos, Address(rsp, 3*BytesPerWord));
3385         __ movptr   (src,     Address(rsp, 4*BytesPerWord));
3386 
3387 
3388         __ subl(length, tmp);
3389         __ addl(src_pos, tmp);
3390         __ addl(dst_pos, tmp);
3391       }
3392 
3393       __ jmp(*stub-&gt;entry());
3394 
3395       __ bind(cont);
3396       __ pop(dst);
3397       __ pop(src);
3398     }
3399   }
3400 
3401 #ifdef ASSERT
3402   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
3403     // Sanity check the known type with the incoming class.  For the
3404     // primitive case the types must match exactly with src.klass and
3405     // dst.klass each exactly matching the default type.  For the
3406     // object array case, if no type check is needed then either the
3407     // dst type is exactly the expected type and the src type is a
3408     // subtype which we can't check or src is the same array as dst
3409     // but not necessarily exactly of type default_type.
3410     Label known_ok, halt;
3411     __ mov_metadata(tmp, default_type-&gt;constant_encoding());
3412 #ifdef _LP64
3413     if (UseCompressedClassPointers) {
3414       __ encode_klass_not_null(tmp);
3415     }
3416 #endif
3417 
3418     if (basic_type != T_OBJECT) {
3419 
3420       if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);
3421       else                   __ cmpptr(tmp, dst_klass_addr);
3422       __ jcc(Assembler::notEqual, halt);
3423       if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);
3424       else                   __ cmpptr(tmp, src_klass_addr);
3425       __ jcc(Assembler::equal, known_ok);
3426     } else {
3427       if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);
3428       else                   __ cmpptr(tmp, dst_klass_addr);
3429       __ jcc(Assembler::equal, known_ok);
3430       __ cmpptr(src, dst);
3431       __ jcc(Assembler::equal, known_ok);
3432     }
3433     __ bind(halt);
3434     __ stop("incorrect type information in arraycopy");
3435     __ bind(known_ok);
3436   }
3437 #endif
3438 
3439 #ifndef PRODUCT
3440   if (PrintC1Statistics) {
3441     __ incrementl(ExternalAddress(Runtime1::arraycopy_count_address(basic_type)));
3442   }
3443 #endif
3444 
3445 #ifdef _LP64
3446   assert_different_registers(c_rarg0, dst, dst_pos, length);
3447   __ lea(c_rarg0, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3448   assert_different_registers(c_rarg1, length);
3449   __ lea(c_rarg1, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3450   __ mov(c_rarg2, length);
3451 
3452 #else
3453   __ lea(tmp, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3454   store_parameter(tmp, 0);
3455   __ lea(tmp, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3456   store_parameter(tmp, 1);
3457   store_parameter(length, 2);
3458 #endif // _LP64
3459 
3460   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
3461   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
3462   const char *name;
3463   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
3464   __ call_VM_leaf(entry, 0);
3465 
3466   __ bind(*stub-&gt;continuation());
3467 }
3468 
3469 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
3470   assert(op-&gt;crc()-&gt;is_single_cpu(),  "crc must be register");
3471   assert(op-&gt;val()-&gt;is_single_cpu(),  "byte value must be register");
3472   assert(op-&gt;result_opr()-&gt;is_single_cpu(), "result must be register");
3473   Register crc = op-&gt;crc()-&gt;as_register();
3474   Register val = op-&gt;val()-&gt;as_register();
3475   Register res = op-&gt;result_opr()-&gt;as_register();
3476 
3477   assert_different_registers(val, crc, res);
3478 
3479   __ lea(res, ExternalAddress(StubRoutines::crc_table_addr()));
3480   __ notl(crc); // ~crc
3481   __ update_byte_crc32(crc, val, res);
3482   __ notl(crc); // ~crc
3483   __ mov(res, crc);
3484 }
3485 
3486 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
3487   Register obj = op-&gt;obj_opr()-&gt;as_register();  // may not be an oop
3488   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
3489   Register lock = op-&gt;lock_opr()-&gt;as_register();
3490   if (!UseFastLocking) {
3491     __ jmp(*op-&gt;stub()-&gt;entry());
3492   } else if (op-&gt;code() == lir_lock) {
3493     Register scratch = noreg;
3494     if (UseBiasedLocking) {
3495       scratch = op-&gt;scratch_opr()-&gt;as_register();
3496     }
3497     assert(BasicLock::displaced_header_offset_in_bytes() == 0, "lock_reg must point to the displaced header");
3498     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
3499     // add debug info for NullPointerException only if one is possible
3500     int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op-&gt;stub()-&gt;entry());
3501     if (op-&gt;info() != NULL) {
3502       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
3503     }
3504     // done
3505   } else if (op-&gt;code() == lir_unlock) {
3506     assert(BasicLock::displaced_header_offset_in_bytes() == 0, "lock_reg must point to the displaced header");
3507     __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
3508   } else {
3509     Unimplemented();
3510   }
3511   __ bind(*op-&gt;stub()-&gt;continuation());
3512 }
3513 
3514 
3515 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
3516   ciMethod* method = op-&gt;profiled_method();
3517   int bci          = op-&gt;profiled_bci();
3518   ciMethod* callee = op-&gt;profiled_callee();
3519 
3520   // Update counter for all call types
3521   ciMethodData* md = method-&gt;method_data_or_null();
3522   assert(md != NULL, "Sanity");
3523   ciProfileData* data = md-&gt;bci_to_data(bci);
3524   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), "need CounterData for calls");
3525   assert(op-&gt;mdo()-&gt;is_single_cpu(),  "mdo must be allocated");
3526   Register mdo  = op-&gt;mdo()-&gt;as_register();
3527   __ mov_metadata(mdo, md-&gt;constant_encoding());
3528   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
3529   // Perform additional virtual call profiling for invokevirtual and
3530   // invokeinterface bytecodes
3531   if (op-&gt;should_profile_receiver_type()) {
3532     assert(op-&gt;recv()-&gt;is_single_cpu(), "recv must be allocated");
3533     Register recv = op-&gt;recv()-&gt;as_register();
3534     assert_different_registers(mdo, recv);
3535     assert(data-&gt;is_VirtualCallData(), "need VirtualCallData for virtual calls");
3536     ciKlass* known_klass = op-&gt;known_holder();
3537     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
3538       // We know the type that will be seen at this call site; we can
3539       // statically update the MethodData* rather than needing to do
3540       // dynamic tests on the receiver type
3541 
3542       // NOTE: we should probably put a lock around this search to
3543       // avoid collisions by concurrent compilations
3544       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
3545       uint i;
3546       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
3547         ciKlass* receiver = vc_data-&gt;receiver(i);
3548         if (known_klass-&gt;equals(receiver)) {
3549           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
3550           __ addptr(data_addr, DataLayout::counter_increment);
3551           return;
3552         }
3553       }
3554 
3555       // Receiver type not found in profile data; select an empty slot
3556 
3557       // Note that this is less efficient than it should be because it
3558       // always does a write to the receiver part of the
3559       // VirtualCallData rather than just the first time
3560       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
3561         ciKlass* receiver = vc_data-&gt;receiver(i);
3562         if (receiver == NULL) {
3563           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));
3564           __ mov_metadata(recv_addr, known_klass-&gt;constant_encoding());
3565           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
3566           __ addptr(data_addr, DataLayout::counter_increment);
3567           return;
3568         }
3569       }
3570     } else {
3571       __ load_klass(recv, recv);
3572       Label update_done;
3573       type_profile_helper(mdo, md, data, recv, &amp;update_done);
3574       // Receiver did not match any saved receiver and there is no empty row for it.
3575       // Increment total counter to indicate polymorphic case.
3576       __ addptr(counter_addr, DataLayout::counter_increment);
3577 
3578       __ bind(update_done);
3579     }
3580   } else {
3581     // Static call
3582     __ addptr(counter_addr, DataLayout::counter_increment);
3583   }
3584 }
3585 
3586 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
3587   Register obj = op-&gt;obj()-&gt;as_register();
3588   Register tmp = op-&gt;tmp()-&gt;as_pointer_register();
3589   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
3590   ciKlass* exact_klass = op-&gt;exact_klass();
3591   intptr_t current_klass = op-&gt;current_klass();
3592   bool not_null = op-&gt;not_null();
3593   bool no_conflict = op-&gt;no_conflict();
3594 
3595   Label update, next, none;
3596 
3597   bool do_null = !not_null;
3598   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
3599   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
3600 
3601   assert(do_null || do_update, "why are we here?");
3602   assert(!TypeEntries::was_null_seen(current_klass) || do_update, "why are we here?");
3603 
3604   __ verify_oop(obj);
3605 
3606   if (tmp != obj) {
3607     __ mov(tmp, obj);
3608   }
3609   if (do_null) {
3610     __ testptr(tmp, tmp);
3611     __ jccb(Assembler::notZero, update);
3612     if (!TypeEntries::was_null_seen(current_klass)) {
3613       __ orptr(mdo_addr, TypeEntries::null_seen);
3614     }
3615     if (do_update) {
3616 #ifndef ASSERT
3617       __ jmpb(next);
3618     }
3619 #else
3620       __ jmp(next);
3621     }
3622   } else {
3623     __ testptr(tmp, tmp);
3624     __ jcc(Assembler::notZero, update);
3625     __ stop("unexpect null obj");
3626 #endif
3627   }
3628 
3629   __ bind(update);
3630 
3631   if (do_update) {
3632 #ifdef ASSERT
3633     if (exact_klass != NULL) {
3634       Label ok;
3635       __ load_klass(tmp, tmp);
3636       __ push(tmp);
3637       __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3638       __ cmpptr(tmp, Address(rsp, 0));
3639       __ jcc(Assembler::equal, ok);
3640       __ stop("exact klass and actual klass differ");
3641       __ bind(ok);
3642       __ pop(tmp);
3643     }
3644 #endif
3645     if (!no_conflict) {
3646       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
3647         if (exact_klass != NULL) {
3648           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3649         } else {
3650           __ load_klass(tmp, tmp);
3651         }
3652 
3653         __ xorptr(tmp, mdo_addr);
3654         __ testptr(tmp, TypeEntries::type_klass_mask);
3655         // klass seen before, nothing to do. The unknown bit may have been
3656         // set already but no need to check.
3657         __ jccb(Assembler::zero, next);
3658 
3659         __ testptr(tmp, TypeEntries::type_unknown);
3660         __ jccb(Assembler::notZero, next); // already unknown. Nothing to do anymore.
3661 
3662         if (TypeEntries::is_type_none(current_klass)) {
3663           __ cmpptr(mdo_addr, 0);
3664           __ jccb(Assembler::equal, none);
3665           __ cmpptr(mdo_addr, TypeEntries::null_seen);
3666           __ jccb(Assembler::equal, none);
3667           // There is a chance that the checks above (re-reading profiling
3668           // data from memory) fail if another thread has just set the
3669           // profiling to this obj's klass
3670           __ xorptr(tmp, mdo_addr);
3671           __ testptr(tmp, TypeEntries::type_klass_mask);
3672           __ jccb(Assembler::zero, next);
3673         }
3674       } else {
3675         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3676                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, "conflict only");
3677 
3678         __ movptr(tmp, mdo_addr);
3679         __ testptr(tmp, TypeEntries::type_unknown);
3680         __ jccb(Assembler::notZero, next); // already unknown. Nothing to do anymore.
3681       }
3682 
3683       // different than before. Cannot keep accurate profile.
3684       __ orptr(mdo_addr, TypeEntries::type_unknown);
3685 
3686       if (TypeEntries::is_type_none(current_klass)) {
3687         __ jmpb(next);
3688 
3689         __ bind(none);
3690         // first time here. Set profile type.
3691         __ movptr(mdo_addr, tmp);
3692       }
3693     } else {
3694       // There's a single possible klass at this profile point
3695       assert(exact_klass != NULL, "should be");
3696       if (TypeEntries::is_type_none(current_klass)) {
3697         __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3698         __ xorptr(tmp, mdo_addr);
3699         __ testptr(tmp, TypeEntries::type_klass_mask);
3700 #ifdef ASSERT
3701         __ jcc(Assembler::zero, next);
3702 
3703         {
3704           Label ok;
3705           __ push(tmp);
3706           __ cmpptr(mdo_addr, 0);
3707           __ jcc(Assembler::equal, ok);
3708           __ cmpptr(mdo_addr, TypeEntries::null_seen);
3709           __ jcc(Assembler::equal, ok);
3710           // may have been set by another thread
3711           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3712           __ xorptr(tmp, mdo_addr);
3713           __ testptr(tmp, TypeEntries::type_mask);
3714           __ jcc(Assembler::zero, ok);
3715 
3716           __ stop("unexpected profiling mismatch");
3717           __ bind(ok);
3718           __ pop(tmp);
3719         }
3720 #else
3721         __ jccb(Assembler::zero, next);
3722 #endif
3723         // first time here. Set profile type.
3724         __ movptr(mdo_addr, tmp);
3725       } else {
3726         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3727                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, "inconsistent");
3728 
3729         __ movptr(tmp, mdo_addr);
3730         __ testptr(tmp, TypeEntries::type_unknown);
3731         __ jccb(Assembler::notZero, next); // already unknown. Nothing to do anymore.
3732 
3733         __ orptr(mdo_addr, TypeEntries::type_unknown);
3734       }
3735     }
3736 
3737     __ bind(next);
3738   }
3739 }
3740 
3741 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
3742   Unimplemented();
3743 }
3744 
3745 
3746 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
3747   __ lea(dst-&gt;as_register(), frame_map()-&gt;address_for_monitor_lock(monitor_no));
3748 }
3749 
3750 
3751 void LIR_Assembler::align_backward_branch_target() {
3752   __ align(BytesPerWord);
3753 }
3754 
3755 
3756 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
3757   if (left-&gt;is_single_cpu()) {
3758     __ negl(left-&gt;as_register());
3759     move_regs(left-&gt;as_register(), dest-&gt;as_register());
3760 
3761   } else if (left-&gt;is_double_cpu()) {
3762     Register lo = left-&gt;as_register_lo();
3763 #ifdef _LP64
3764     Register dst = dest-&gt;as_register_lo();
3765     __ movptr(dst, lo);
3766     __ negptr(dst);
3767 #else
3768     Register hi = left-&gt;as_register_hi();
3769     __ lneg(hi, lo);
3770     if (dest-&gt;as_register_lo() == hi) {
3771       assert(dest-&gt;as_register_hi() != lo, "destroying register");
3772       move_regs(hi, dest-&gt;as_register_hi());
3773       move_regs(lo, dest-&gt;as_register_lo());
3774     } else {
3775       move_regs(lo, dest-&gt;as_register_lo());
3776       move_regs(hi, dest-&gt;as_register_hi());
3777     }
3778 #endif // _LP64
3779 
3780   } else if (dest-&gt;is_single_xmm()) {
3781 #ifdef _LP64
3782     if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
3783       assert(tmp-&gt;is_valid(), "need temporary");
3784       assert_different_registers(left-&gt;as_xmm_float_reg(), tmp-&gt;as_xmm_float_reg());
3785       __ vpxor(dest-&gt;as_xmm_float_reg(), tmp-&gt;as_xmm_float_reg(), left-&gt;as_xmm_float_reg(), 2);
3786     }
3787     else
3788 #endif
3789     {
3790       assert(!tmp-&gt;is_valid(), "do not need temporary");
3791       if (left-&gt;as_xmm_float_reg() != dest-&gt;as_xmm_float_reg()) {
3792         __ movflt(dest-&gt;as_xmm_float_reg(), left-&gt;as_xmm_float_reg());
3793       }
3794       __ xorps(dest-&gt;as_xmm_float_reg(),
3795                ExternalAddress((address)float_signflip_pool));
3796     }
3797   } else if (dest-&gt;is_double_xmm()) {
3798 #ifdef _LP64
3799     if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
3800       assert(tmp-&gt;is_valid(), "need temporary");
3801       assert_different_registers(left-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg());
3802       __ vpxor(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg(), 2);
3803     }
3804     else
3805 #endif
3806     {
3807       assert(!tmp-&gt;is_valid(), "do not need temporary");
3808       if (left-&gt;as_xmm_double_reg() != dest-&gt;as_xmm_double_reg()) {
3809         __ movdbl(dest-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg());
3810       }
3811       __ xorpd(dest-&gt;as_xmm_double_reg(),
3812                ExternalAddress((address)double_signflip_pool));
3813     }
3814   } else if (left-&gt;is_single_fpu() || left-&gt;is_double_fpu()) {
3815     assert(left-&gt;fpu() == 0, "arg must be on TOS");
3816     assert(dest-&gt;fpu() == 0, "dest must be TOS");
3817     __ fchs();
3818 
3819   } else {
3820     ShouldNotReachHere();
3821   }
3822 }
3823 
3824 
3825 void LIR_Assembler::leal(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
3826   assert(src-&gt;is_address(), "must be an address");
3827   assert(dest-&gt;is_register(), "must be a register");
3828 
3829   PatchingStub* patch = NULL;
3830   if (patch_code != lir_patch_none) {
3831     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
3832   }
3833 
3834   Register reg = dest-&gt;as_pointer_register();
3835   LIR_Address* addr = src-&gt;as_address_ptr();
3836   __ lea(reg, as_Address(addr));
3837 
3838   if (patch != NULL) {
3839     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
3840   }
3841 }
3842 
3843 
3844 
3845 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
3846   assert(!tmp-&gt;is_valid(), "don't need temporary");
3847   __ call(RuntimeAddress(dest));
3848   if (info != NULL) {
3849     add_call_info_here(info);
3850   }
3851 }
3852 
3853 
3854 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
3855   assert(type == T_LONG, "only for volatile long fields");
3856 
3857   if (info != NULL) {
3858     add_debug_info_for_null_check_here(info);
3859   }
3860 
3861   if (src-&gt;is_double_xmm()) {
3862     if (dest-&gt;is_double_cpu()) {
3863 #ifdef _LP64
3864       __ movdq(dest-&gt;as_register_lo(), src-&gt;as_xmm_double_reg());
3865 #else
3866       __ movdl(dest-&gt;as_register_lo(), src-&gt;as_xmm_double_reg());
3867       __ psrlq(src-&gt;as_xmm_double_reg(), 32);
3868       __ movdl(dest-&gt;as_register_hi(), src-&gt;as_xmm_double_reg());
3869 #endif // _LP64
3870     } else if (dest-&gt;is_double_stack()) {
3871       __ movdbl(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()), src-&gt;as_xmm_double_reg());
3872     } else if (dest-&gt;is_address()) {
3873       __ movdbl(as_Address(dest-&gt;as_address_ptr()), src-&gt;as_xmm_double_reg());
3874     } else {
3875       ShouldNotReachHere();
3876     }
3877 
3878   } else if (dest-&gt;is_double_xmm()) {
3879     if (src-&gt;is_double_stack()) {
3880       __ movdbl(dest-&gt;as_xmm_double_reg(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
3881     } else if (src-&gt;is_address()) {
3882       __ movdbl(dest-&gt;as_xmm_double_reg(), as_Address(src-&gt;as_address_ptr()));
3883     } else {
3884       ShouldNotReachHere();
3885     }
3886 
3887   } else if (src-&gt;is_double_fpu()) {
3888     assert(src-&gt;fpu_regnrLo() == 0, "must be TOS");
3889     if (dest-&gt;is_double_stack()) {
3890       __ fistp_d(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
3891     } else if (dest-&gt;is_address()) {
3892       __ fistp_d(as_Address(dest-&gt;as_address_ptr()));
3893     } else {
3894       ShouldNotReachHere();
3895     }
3896 
3897   } else if (dest-&gt;is_double_fpu()) {
3898     assert(dest-&gt;fpu_regnrLo() == 0, "must be TOS");
3899     if (src-&gt;is_double_stack()) {
3900       __ fild_d(frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
3901     } else if (src-&gt;is_address()) {
3902       __ fild_d(as_Address(src-&gt;as_address_ptr()));
3903     } else {
3904       ShouldNotReachHere();
3905     }
3906   } else {
3907     ShouldNotReachHere();
3908   }
3909 }
3910 
3911 #ifdef ASSERT
3912 // emit run-time assertion
3913 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
3914   assert(op-&gt;code() == lir_assert, "must be");
3915 
3916   if (op-&gt;in_opr1()-&gt;is_valid()) {
3917     assert(op-&gt;in_opr2()-&gt;is_valid(), "both operands must be valid");
3918     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
3919   } else {
3920     assert(op-&gt;in_opr2()-&gt;is_illegal(), "both operands must be illegal");
3921     assert(op-&gt;condition() == lir_cond_always, "no other conditions allowed");
3922   }
3923 
3924   Label ok;
3925   if (op-&gt;condition() != lir_cond_always) {
3926     Assembler::Condition acond = Assembler::zero;
3927     switch (op-&gt;condition()) {
3928       case lir_cond_equal:        acond = Assembler::equal;       break;
3929       case lir_cond_notEqual:     acond = Assembler::notEqual;    break;
3930       case lir_cond_less:         acond = Assembler::less;        break;
3931       case lir_cond_lessEqual:    acond = Assembler::lessEqual;   break;
3932       case lir_cond_greaterEqual: acond = Assembler::greaterEqual;break;
3933       case lir_cond_greater:      acond = Assembler::greater;     break;
3934       case lir_cond_belowEqual:   acond = Assembler::belowEqual;  break;
3935       case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;  break;
3936       default:                    ShouldNotReachHere();
3937     }
3938     __ jcc(acond, ok);
3939   }
3940   if (op-&gt;halt()) {
3941     const char* str = __ code_string(op-&gt;msg());
3942     __ stop(str);
3943   } else {
3944     breakpoint();
3945   }
3946   __ bind(ok);
3947 }
3948 #endif
3949 
3950 void LIR_Assembler::membar() {
3951   // QQQ sparc TSO uses this,
3952   __ membar( Assembler::Membar_mask_bits(Assembler::StoreLoad));
3953 }
3954 
3955 void LIR_Assembler::membar_acquire() {
3956   // No x86 machines currently require load fences
3957 }
3958 
3959 void LIR_Assembler::membar_release() {
3960   // No x86 machines currently require store fences
3961 }
3962 
3963 void LIR_Assembler::membar_loadload() {
3964   // no-op
3965   //__ membar(Assembler::Membar_mask_bits(Assembler::loadload));
3966 }
3967 
3968 void LIR_Assembler::membar_storestore() {
3969   // no-op
3970   //__ membar(Assembler::Membar_mask_bits(Assembler::storestore));
3971 }
3972 
3973 void LIR_Assembler::membar_loadstore() {
3974   // no-op
3975   //__ membar(Assembler::Membar_mask_bits(Assembler::loadstore));
3976 }
3977 
3978 void LIR_Assembler::membar_storeload() {
3979   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
3980 }
3981 
3982 void LIR_Assembler::on_spin_wait() {
3983   __ pause ();
3984 }
3985 
3986 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
3987   assert(result_reg-&gt;is_register(), "check");
3988 #ifdef _LP64
3989   // __ get_thread(result_reg-&gt;as_register_lo());
3990   __ mov(result_reg-&gt;as_register(), r15_thread);
3991 #else
3992   __ get_thread(result_reg-&gt;as_register());
3993 #endif // _LP64
3994 }
3995 
3996 
3997 void LIR_Assembler::peephole(LIR_List*) {
3998   // do nothing for now
3999 }
4000 
4001 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
4002   assert(data == dest, "xchg/xadd uses only 2 operands");
4003 
4004   if (data-&gt;type() == T_INT) {
4005     if (code == lir_xadd) {
4006       __ lock();
4007       __ xaddl(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register());
4008     } else {
4009       __ xchgl(data-&gt;as_register(), as_Address(src-&gt;as_address_ptr()));
4010     }
4011   } else if (data-&gt;is_oop()) {
4012     assert (code == lir_xchg, "xadd for oops");
4013     Register obj = data-&gt;as_register();
4014 #ifdef _LP64
4015     if (UseCompressedOops) {
4016       __ encode_heap_oop(obj);
4017       __ xchgl(obj, as_Address(src-&gt;as_address_ptr()));
4018       __ decode_heap_oop(obj);
4019     } else {
4020       __ xchgptr(obj, as_Address(src-&gt;as_address_ptr()));
4021     }
4022 #else
4023     __ xchgl(obj, as_Address(src-&gt;as_address_ptr()));
4024 #endif
4025   } else if (data-&gt;type() == T_LONG) {
4026 #ifdef _LP64
4027     assert(data-&gt;as_register_lo() == data-&gt;as_register_hi(), "should be a single register");
4028     if (code == lir_xadd) {
4029       __ lock();
4030       __ xaddq(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register_lo());
4031     } else {
4032       __ xchgq(data-&gt;as_register_lo(), as_Address(src-&gt;as_address_ptr()));
4033     }
4034 #else
4035     ShouldNotReachHere();
4036 #endif
4037   } else {
4038     ShouldNotReachHere();
4039   }
4040 }
4041 
4042 #undef __
</pre></body></html>
